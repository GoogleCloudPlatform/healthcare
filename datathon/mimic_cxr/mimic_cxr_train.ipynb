{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e4TNC-3LszlF",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "# Training a Convolutional Neural Network to Classify Chest X-rays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JsNl7d9Cy6X4",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T5_Riz79y_ZO",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "This notebook shows how to train a state of the art Convolutional Neural Network\n",
        "(CNN) to classify chest X-rays images from the MIMIC CXR Dataset. Its approach\n",
        "is influenced by [CheXpert: A Large Chest Radiograph Dataset with Uncertainty\n",
        "Labels and Expert Comparison](https://arxiv.org/abs/1901.07031).\n",
        "\n",
        "You can run this notebook from [Colab](https://colab.research.google.com/) or\n",
        "[Cloud AI Platform Notebook](https://cloud.google.com/ai-platform-notebooks/).\n",
        "If you're serious about training your own models, you'll definitely want to use\n",
        "a Cloud AI Platform notebook with one or more TPUs or GPUs. If you're just\n",
        "interested in learning how to train a CNN, you can run this notebook in Colab.\n",
        "Your Colab session will probably timeout before it can finish training the model\n",
        "(Cloud AI Platform notebooks are more powerful and never timeout). In any case,\n",
        "don't worry, several pretrained models are available along with the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-m30o577n9mt",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "# Use tensorflow 1.14 (should be released very soon)\n",
        "# this notebook is also compatible with tensorflow 2.0\n",
        "!pip install -q tf-nightly-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hwfrAocTs92n",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import multiprocessing\n",
        "from enum import Enum\n",
        "from google.cloud import bigquery\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    from google.colab import auth\n",
        "    IN_COLAB = True\n",
        "    auth.authenticate_user()\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJ7qFWUisQJD",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Understanding the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L2RbPSlZ-bqi",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "First, we need to specify where the training and validation datasets are\n",
        "located. Labelled images are provided in\n",
        "[TFRecord](https://www.tensorflow.org/guide/datasets#consuming_tfrecord_data)\n",
        "format. TFRecords are a great choice for performant and convenient training. You\n",
        "also have access to BigQuery tables that contain the labels for each image,\n",
        "which we'll use to get a broad understanding of how the labels are distributed\n",
        "before we dive into training our model.\n",
        "\n",
        "There are separate TFRecords for X-rays taken from frontal or lateral views. You\n",
        "can choose either type of dataset, but make sure the validation and training\n",
        "dataset correspond to the same view. There are pretrained models available for\n",
        "both views."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "s6i-MM779FbU",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "#@title Input Datasets {run: \"auto\"}\n",
        "GCP_ANALYSIS_PROJECT = 'your-analysis-project'  #@param {type: \"string\"}\n",
        "TRAIN_TFRECORDS = 'gs://your-analysis-bucket/mimic-cxr-processed/resized_tfrecord/train/frontal*'  #@param {type: \"string\"}\n",
        "VALID_TFRECORDS = 'gs://your-analysis-bucket/mimic-cxr-processed/resized_tfrecord/valid/frontal*'  #@param {type: \"string\"}\n",
        "# VIEW should be one of 'frontal', or 'lateral'\n",
        "VIEW = 'frontal'  #@param [\"frontal\", \"lateral\"] {type: \"string\"}\n",
        "TRAIN_BIGQUERY = 'your-analysis-project.cxr.processed_labels_train'  #@param {type: \"string\"}\n",
        "VALID_BIGQUERY = 'your-analysis-project.cxr.processed_labels_valid'  #@param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vaN3NKPk-3s8",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "The dataset consists of labelled images. These labels were determined by\n",
        "analyzing radiologist notes. A label is given the value of\n",
        "\n",
        "*   `0` (`not_mentioned`) if the note made no mention of it\n",
        "*   `1` (`negative`) if the note said the label wasn't present in the image\n",
        "*   `2` (`uncertain`) if the note expressed uncertainty about the label's\n",
        "    presence\n",
        "*   `3` (`positive`) if the label was mentioned with certainty in the note\n",
        "\n",
        "for more details about how these labels were generated, you can check out\n",
        "[this paper](https://arxiv.org/abs/1901.07031). For our classifier we'll treat\n",
        "`not_mentioned` (`0`) and `negative` (`1`) as the same thing. There's some\n",
        "choice in how we handle `uncertain` (`2`). We will investigate the uncertain labels in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4R8Amh9EyNvG",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "class Labels(Enum):\n",
        "  no_finding = 0\n",
        "  enlarged_cardiomediastinum = 1\n",
        "  cardiomegaly = 2\n",
        "  airspace_opacity = 3\n",
        "  lung_lesion = 4\n",
        "  edema = 5\n",
        "  consolidation = 6\n",
        "  pneumonia = 7\n",
        "  atelectasis = 8\n",
        "  pneumothorax = 9\n",
        "  pleural_effusion = 10\n",
        "  pleural_other = 11\n",
        "  fracture = 12\n",
        "  support_devices = 13\n",
        "\n",
        "\n",
        "class LabelValues(Enum):\n",
        "  not_mentioned = 0\n",
        "  negative = 1\n",
        "  uncertain = 2\n",
        "  positive = 3\n",
        "\n",
        "\n",
        "class Views(Enum):\n",
        "  frontal = 0\n",
        "  lateral = 1\n",
        "  other = 2\n",
        "\n",
        "\n",
        "class Datasets(Enum):\n",
        "  train = 0\n",
        "  valid = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "csxgv426CP6B",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "Before we start building our model, let's check out the distribution of the\n",
        "data. We'll do this by writing a BigQuery StandardSQL statement that counts the\n",
        "number of `not_mentioned`, `negative`, `uncertain` and `positive` values for\n",
        "each label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uYoxsUQvEV6_",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "bq_client = bigquery.Client(project=GCP_ANALYSIS_PROJECT)\n",
        "\n",
        "queries = []\n",
        "for label in Labels:\n",
        "  queries.append(\"\"\"\n",
        "    SELECT\n",
        "      \"{label}\" AS label,\n",
        "      {label} AS label_value,\n",
        "      COUNT(DISTINCT path) AS cnt,\n",
        "      dataset\n",
        "    FROM\n",
        "      (SELECT * FROM `{TRAIN_BIGQUERY}`\n",
        "      UNION ALL\n",
        "      SELECT * FROM `{VALID_BIGQUERY}`)\n",
        "    WHERE view = {view_value}\n",
        "    GROUP BY {label}, dataset\n",
        "    \"\"\".format(\n",
        "        TRAIN_BIGQUERY=TRAIN_BIGQUERY,\n",
        "        VALID_BIGQUERY=VALID_BIGQUERY,\n",
        "        label=label.name,\n",
        "        view_value=Views[VIEW].value))\n",
        "\n",
        "barplot_df = bq_client.query('UNION ALL'.join(queries)).to_dataframe()\n",
        "# Convert integer label values into strings\n",
        "barplot_df.label_value = barplot_df.label_value.apply(lambda v: LabelValues(v).\n",
        "                                                      name)\n",
        "barplot_df.dataset = barplot_df.dataset.apply(lambda v: Datasets(v).name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lqxqYe1Z27wR",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "Our StandardSQL statement returns a pandas Dataframe in 'long' format, which\n",
        "makes it really easy to visualize with [seaborn](https://seaborn.pydata.org/)\n",
        "(or [ggplot2](https://ggplot2.tidyverse.org/) for R users)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9qNznWt9Dzbg",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "sns.catplot(\n",
        "    y='label',\n",
        "    x='cnt',\n",
        "    hue='label_value',\n",
        "    data=barplot_df,\n",
        "    col='dataset',\n",
        "    kind='bar',\n",
        "    sharex=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OmW0g8kI7JHR",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "N_TRAIN = barplot_df.cnt[(barplot_df.dataset == 'train')\n",
        "                         \u0026 (barplot_df.label == Labels(0).name)].sum()\n",
        "N_VALID = barplot_df.cnt[(barplot_df.dataset == 'valid')\n",
        "                         \u0026 (barplot_df.label == Labels(0).name)].sum()\n",
        "print('training examples: {:,}\\nvalidation examples: {:,}'.format(\n",
        "    N_TRAIN, N_VALID))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vzyafcOWE7uF",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "What have we learned from this?\n",
        "\n",
        "*   we have a medium sized dataset for training a CNN. For comparison\n",
        "    [ImageNet](http://www.image-net.org/) has 14 million images, and\n",
        "    [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) has 60,000 images.\n",
        "*   the distribution of the values varies for each label, the only constant is\n",
        "    that the majority of values are `not_mentioned`.\n",
        "\n",
        "This informs our expectations for any model's performance with each label. It\n",
        "also suggests that we find a way to take advantage of the `uncertain` labels,\n",
        "since in some cases they actually outnumber the `positive` labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F1-b9NXMyykj",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Creating an input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UeKhixlozB9w",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "### Performance considerations\n",
        "\n",
        "One of the most important factors in determining how long it takes to train a\n",
        "model is the process that loads data into the model. TPUs and GPUs are so fast\n",
        "that keeping them busy with the next batch of data isn't easy.\n",
        "\n",
        "A few tips for fast input pipelines:\n",
        "\n",
        "1.  Use TFRecords: they allow for data to be read in contiguous blocks, which is\n",
        "    much faster than reading a bunch of small files.\n",
        "1.  Train your models and store your data in the cloud so you can take advantage\n",
        "    of Google's fast internal networks.\n",
        "1.  Perform expensive transformations, including resizing large images, ahead of\n",
        "    time. We've already done this for you using\n",
        "    [Cloud Dataflow](https://cloud.google.com/dataflow/).\n",
        "1.  Use the largest batch size that will fit in your device's memory.\n",
        "\n",
        "You can find more tips\n",
        "[here](https://www.tensorflow.org/alpha/guide/data_performance). And if you're\n",
        "using a cloud TPU, you can also use the\n",
        "[`cloud_tpu_profiler`](https://cloud.google.com/tpu/docs/cloud-tpu-tools#profile_tab),\n",
        "which is an incredibly helpful tool for improving your model's performance.\n",
        "\n",
        "### Dealing with uncertain labels (advanced)\n",
        "\n",
        "Our second issue is how to assign values to the uncertain labels. This is a\n",
        "little technical, so feel free to skim through this section.\n",
        "\n",
        "Almost all multi-label neural networks use a loss function like this:\n",
        "\n",
        "$$\n",
        "L = - \\sum_{n,i} l\\left(y_{ni}, t_{ni}\\right)\n",
        "$$\n",
        "\n",
        "Where $t_{ni}$ is the true value of the $i^\\text{th}$ label of the $n^\\text{th}$\n",
        "sample and $y_{ni}$ is the corresponding prediction from our model.\n",
        "\n",
        "One way to incorporate uncertain labels is to assign them a value $u \\in [0, 1]$\n",
        "and a weight $w \\in [0, 1]$ to that our loss function becomes\n",
        "\n",
        "$$\n",
        "L =  - \\sum_{n,i} \\begin{cases}\n",
        "l\\left(y_{ni}, t_{ni}\\right) \u0026 (n, i)^{\\text{th}} \\text{ label is certain} \\\\\n",
        "w \\cdot l\\left(y_{ni}, u\\right) \u0026 (n, i)^{\\text{th}} \\text{ label is uncertain}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$w = 0$ corresponds to ignoring the uncertain labels. $w = 1, u = 0$ corresponds\n",
        "to counting all uncertain labels as negative. $w = 1, u = 1$ counts the\n",
        "uncertain labels as positive. Something like $w = 0.5, u = 0.25$ is a hybrid of\n",
        "the others. You can think of $w$ as playing the role of $\\sigma^2$ if $l$ was\n",
        "the log-likelihood of a normal distribution.\n",
        "\n",
        "Other approaches for incorporating uncertain labels into a model are discussed\n",
        "[here](https://arxiv.org/abs/1901.07031). You could also experiment with using\n",
        "different values of $u$ and $w$ for each label. You could even try to optimize\n",
        "these hyperparameters. The per label values of $u$ for example could be learned\n",
        "with gradient descent, while $w$ could be updated every epoch to optimize the\n",
        "loss on the `certain` labels.\n",
        "\n",
        "In summary:\n",
        "\n",
        "*   `U_VALUE` ($u$) is the probability of being `positive` that you assign to\n",
        "    uncertain labels\n",
        "*   `W_VALUE` ($w$) is the weight that you assign to uncertain labels during\n",
        "    training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "udjTuUIy_wUy",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "#@title Input pipeline parameters {run: \"auto\"}\n",
        "BATCH_SIZE = 32  #@param {type: \"integer\"}\n",
        "NUM_EPOCHS = 3  #@param {type: \"integer\"}\n",
        "U_VALUE = 0.4  #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "W_VALUE = 0.75  #@param {type:\"slider\", min:0, max:1, step:0.01}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ctKPP8DXAHfS",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "# label -\u003e probability table: 0 -\u003e 0, 1 -\u003e 0, 2 -\u003e u, 3 -\u003e 1\n",
        "probabs_lookup = tf.constant([0.0, 0.0, U_VALUE, 1.0])\n",
        "# label -\u003e weight table: 0 -\u003e 1, 1 -\u003e 1, 2 -\u003e w, 3 -\u003e 1\n",
        "weights_lookup = tf.constant([1.0, 1.0, W_VALUE, 1.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "q_4UP-Wwyx3t",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "feature_description = {'jpg_bytes': tf.io.FixedLenFeature([], tf.string)}\n",
        "for l in Labels:\n",
        "  feature_description[l.name] = tf.io.FixedLenFeature([], tf.int64)\n",
        "\n",
        "# The height, width, and number of channels of the input images\n",
        "INPUT_HWC = (320, 320, 1)\n",
        "\n",
        "\n",
        "def parse_function(example):\n",
        "  \"\"\"Convert a TFExample from a TFRecord into an input and its true label.\n",
        "\n",
        "    Args:\n",
        "      example (tf.train.Example): A training example read from a TFRecord.\n",
        "\n",
        "    Returns:\n",
        "      Tuple[tf.Tensor, tf.Tensor]: The X-ray image and its labels. The labels\n",
        "        are represented as two stacked arrays. One array is the probability\n",
        "        that this label exists in the image, the other is how much weight this\n",
        "        label should have when training the model.\n",
        "    \"\"\"\n",
        "  parsed = tf.io.parse_single_example(example, feature_description)\n",
        "  # Turn the JPEG data into a matrix of pixel intensities\n",
        "  image = tf.io.decode_jpeg(parsed['jpg_bytes'], channels=1)\n",
        "  # Give the image a definite size, which is needed by TPUs\n",
        "  image = tf.reshape(image, INPUT_HWC)\n",
        "  # Normalize the pixel values to be between 0 and 1\n",
        "  scaled_image = (1.0 / 255.0) * tf.cast(image, tf.float32)\n",
        "  # Combine the labels into an array\n",
        "  labels = tf.stack([parsed[l.name] for l in Labels], axis=0)\n",
        "  # Convert the labels into probabilities and weights using lookup tables.\n",
        "  probs = tf.gather(probabs_lookup, labels)\n",
        "  weights = tf.gather(weights_lookup, labels)\n",
        "  # Return the input to the model and the true labels\n",
        "  return scaled_image, tf.stack([probs, weights], axis=0)\n",
        "\n",
        "\n",
        "def get_dataset(valid=False):\n",
        "  \"\"\"Construct a pipeline for loading the data.\n",
        "\n",
        "    Args:\n",
        "      valid (bool): If this is True, use the validation dataset instead of the\n",
        "        training dataset.\n",
        "\n",
        "    Returns:\n",
        "      tf.data.Dataset: A dataset loading pipeline ready for training.\n",
        "    \"\"\"\n",
        "  n_cpu = multiprocessing.cpu_count()\n",
        "  tf_records = VALID_TFRECORDS if valid else TRAIN_TFRECORDS\n",
        "  dataset = tf.data.TFRecordDataset(\n",
        "      tf.io.gfile.glob(tf_records),\n",
        "      buffer_size=16 * 1024 * 1024,\n",
        "      num_parallel_reads=n_cpu)\n",
        "  if not valid:\n",
        "    dataset = dataset.shuffle(256)\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.map(parse_function, num_parallel_calls=n_cpu)\n",
        "  dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-iFsTYJNT7mA",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Using accelerators (TPUs and GPUs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rG3LEZ_Jmdcj",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "One of the most exciting things about training neural networks on the cloud is\n",
        "the ability to scale your compute power. Tensorflow makes it easy to scale from\n",
        "a single GPU to a multi-GPU system, to a network of distributed GPU systems.\n",
        "TPUs are even easier. Since all TPUs are distributed systems (with 8 cores and 4\n",
        "chips per board), you can scale from a single TPU to a TPU pod without changing\n",
        "any of your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "798l9Ht1UAOd",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "#@title Accelerators {run: \"auto\"}\n",
        "ACCELERATOR_TYPE = 'Single-GPU'  #@param [\"Single/Multi-TPU\", \"Single-GPU\", \"Multi-GPU\", \"CPU\"] {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iKFe1cWM7FB8",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "if ACCELERATOR_TYPE == 'Single/Multi-TPU':\n",
        "  if IN_COLAB:\n",
        "    tpu_name = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  else:\n",
        "    tpu_name = os.environ['TPU_NAME']\n",
        "  resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_name)\n",
        "  tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "  strategy = tf.contrib.distribute.TPUStrategy(resolver, steps_per_run=100)\n",
        "elif ACCELERATOR_TYPE == 'Multi-GPU':\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy()  # Default strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8udz2vQ_QhWK",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Defining the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9etia5dIQr2d",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "The [keras applications module](https://keras.io/applications/) offers several\n",
        "different CNN architectures for us to choose from, all of which are very good.\n",
        "\n",
        "All we have to do is add the last layer which produces a value for each of our\n",
        "labels. Notice that the activation function for this layer is `'linear'`. That's\n",
        "because our loss function applies its own `sigmoid` nonlinearity to `y_pred`.\n",
        "\n",
        "We define a custom loss function, that unpacks the true probabilities and\n",
        "assigned weights from our input pipeline before using these to compute a\n",
        "weighted cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "i0YyPiGB9vxj",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  base_model = tf.keras.applications.densenet.DenseNet121(\n",
        "      include_top=False, weights=None, input_shape=INPUT_HWC, pooling='max')\n",
        "\n",
        "  predictions = tf.keras.layers.Dense(\n",
        "      len(Labels), activation='linear')(\n",
        "          base_model.output)\n",
        "\n",
        "  model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "  def weighted_binary_crossentropy(prob_weight_y_true, y_pred):\n",
        "    \"\"\"Binary cross-entropy loss function with per-sample weights.\"\"\"\n",
        "    prob_weight_y_true = tf.reshape(prob_weight_y_true, (-1, 2, len(Labels)))\n",
        "    # Unpack the second output of our data pipeline into true probabilities and\n",
        "    # weights for each label.\n",
        "    probs = prob_weight_y_true[:, 0]\n",
        "    weights = prob_weight_y_true[:, 1]\n",
        "    return tf.compat.v1.losses.sigmoid_cross_entropy(\n",
        "        probs,\n",
        "        y_pred,\n",
        "        weights,\n",
        "        reduction=tf.compat.v1.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=tf.train.AdamOptimizer(),\n",
        "      loss=weighted_binary_crossentropy,\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4zuXzbe4tGGv",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QBagIQwEAs8z",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "You can keep track of your loss function and weights during training by saving\n",
        "the training log files to Google Cloud Storage and running a\n",
        "[TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard)\n",
        "session in a Google Cloud Shell or on your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "eCu8LYTxA0tJ",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "#@title GCS Tensorboard log directory {run: \"auto\"}\n",
        "GCS_LOGS = 'gs://your-analysis-bucket/tensorboard/mimic_cxr/'  #@param {\"type\": \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EplL6hJcmoMV",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "Training your model will take hours. You don't have time to waste, so feel free\n",
        "to stop training at any time. You can do this by interrupting the execution of\n",
        "the below cell, either by pressing `âŒ˜/Ctrl+m i` on your keyboard or by clicking\n",
        "the stop sign in the upper left corner with your mouse.\n",
        "\n",
        "We've already trained and generated predictions for several models that you can\n",
        "use throughout the datathon. We'll show you how to use them in the next\n",
        "sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ArdQITx5BAVm",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "now_str = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "callbacks = []\n",
        "if GCS_LOGS:\n",
        "  LOGDIR = GCS_LOGS + ('' if GCS_LOGS.endswith('/') else '/') + now_str\n",
        "  callbacks.append(tf.keras.callbacks.TensorBoard(LOGDIR, update_freq=500))\n",
        "  print('Run `tensorboard --logdir {}` in cloud shell or your local machine.'\n",
        "        .format(GCS_LOGS))\n",
        "\n",
        "model.fit(\n",
        "    get_dataset(),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    steps_per_epoch=N_TRAIN // BATCH_SIZE,\n",
        "    validation_data=get_dataset(valid=True),\n",
        "    validation_steps=N_VALID // BATCH_SIZE,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O3TQzOPGtQwV",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OE6nSWMsC6fs",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "Let's save our model and copy it to Google Cloud Storage.\n",
        "\n",
        "First we'll give our model a descriptive name and decide where to put it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YuAurxVe_nqW",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "#@title Saved model files {run: \"auto\"}\n",
        "MODEL_NAME = 'my_model.h5'  #@param {type: \"string\"}\n",
        "GCS_SAVED_MODEL_DIR = 'gs://your-analysis-bucket/mimic-cxr-models'  #@param {type: \"string\"}\n",
        "GCS_MODEL_PATH = GCS_SAVED_MODEL_DIR\n",
        "if not GCS_MODEL_PATH.endswith('/'):\n",
        "  GCS_MODEL_PATH += '/'\n",
        "GCS_MODEL_PATH += MODEL_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AdPjQPF5uh26",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "Since `tf.data` and `tf.io.gfile` treat files stored in Google Cloud Storage the\n",
        "same as if they were on your local hard drive, uploading and downloading to GCS\n",
        "is the same as reading and writing from your disk in python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ei4Pa16psimM",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "def upload_to_gcs(local_path, gcs_path):\n",
        "  with tf.io.gfile.GFile(gcs_path, 'wb') as gcs_file:\n",
        "    with tf.io.gfile.GFile(local_path, 'rb') as local_file:\n",
        "      gcs_file.write(local_file.read())\n",
        "\n",
        "\n",
        "def download_from_gcs(gcs_path, local_path):\n",
        "  with tf.io.gfile.GFile(gcs_path, 'rb') as gcs_file:\n",
        "    with tf.io.gfile.GFile(local_path, 'wb') as local_file:\n",
        "      local_file.write(gcs_file.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OAUDT4QvpaK7",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "`model.save` save the entire model, including its architecture, learned weights,\n",
        "optimizer, and loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "teO8ts3jAohN",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "model.save(MODEL_NAME)\n",
        "upload_to_gcs(MODEL_NAME, GCS_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EsCwiZLTzDfN",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Making and evaluating predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PI81sHUDgy7A",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "We've used this notebook to pretrain eight different models for you\n",
        "to experiment with.\n",
        "\n",
        "We trained these models on two different image views and four different\n",
        "imputation strategies.\n",
        "\n",
        "*   View:\n",
        "    *   frontal\n",
        "    *   lateral\n",
        "*   Strategy for uncertain labels, $u$, $w$:\n",
        "    *   Ignore uncertain labels, $u = 0$, $w = 0$\n",
        "    *   Assign uncertain labels to negative, $u = 0$, $w = 1$\n",
        "    *   Assign uncertain labels to positive, $u = 1$, $w = 1$\n",
        "    *   Hybrid, $u = 0.5$, $w = 0.25$\n",
        "\n",
        "These pretrained keras models are hosted in Google Cloud Storage, and are\n",
        "available for you to use throughout the datathon.\n",
        "\n",
        "The predictions given by each model are also available in BigQuery. We suggest\n",
        "using these if your team is interested in combining multiple viewpoints or\n",
        "building an ensemble model.\n",
        "\n",
        "First, choose the location of your pretrained keras model on GCS and its table\n",
        "of predictions in BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Xe6-vX_3ob7a",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "PRETRAINED_KERAS_MODEL = 'gs://your-analysis-bucket/mimic-cxr-models/my_model.h5'  #@param {type: \"string\"}\n",
        "VALID_PREDICTIONS_BIGQUERY = 'your-analysis-project.cxr.test_inference_valid'  #@param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DzI75fUeA1yS",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "Then we'll download and load the keras model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3hixJb9dG4_K",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "# Download the model from GCS\n",
        "download_from_gcs(PRETRAINED_KERAS_MODEL, 'pretrained_model.h5')\n",
        "\n",
        "# Load the model, strategy.scope allows us to exploit multiple accelerators,\n",
        "# just like we did during training.\n",
        "with strategy.scope():\n",
        "  model = tf.keras.models.load_model('pretrained_model.h5', compile=False)\n",
        "  model.compile(\n",
        "      optimizer=tf.train.AdamOptimizer(), loss=weighted_binary_crossentropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SbRKVYi2A6RD",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "We can make predictions with this Keras model by calling\n",
        "[`.predict()`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#predict).\n",
        "`.predict()` accepts several different input data types including\n",
        "`tf.data.Dataset`, `np.array`, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4p7629BQ6mPP",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "example_predictions = model.predict(get_dataset(valid=False), steps=3)\n",
        "print('shape: {}, min: {}, max: {}'.format(example_predictions.shape,\n",
        "                                           example_predictions.min(),\n",
        "                                           example_predictions.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Oxk0fURkmt88",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "To compare the model's accuracy against the true labels, let's write a SQL query\n",
        "to join the predicted and true labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1_6FLeJNP1sR",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "pred_true_df = bq_client.query(\"\"\"SELECT * FROM\n",
        "(SELECT {} FROM `{}`)\n",
        "INNER JOIN\n",
        "(SELECT {} FROM `{}`)\n",
        "USING (path)\n",
        "\"\"\".format(\n",
        "    'path, ' + ', '.join('{0} AS predicted_{0}'.format(l.name) for l in Labels),\n",
        "    VALID_PREDICTIONS_BIGQUERY,\n",
        "    'path, ' + ', '.join('{0} AS true_{0}'.format(l.name) for l in Labels),\n",
        "    VALID_BIGQUERY)).to_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P7kFknQdnGyg",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "With this dataframe, we can plot the precision-recall and ROC curves for each\n",
        "label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "yCZ-s2eAkQ4p",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "#@title Evaluation {run: \"auto\"}\n",
        "label = 'edema'  #@param ['no_finding', 'enlarged_cardiomediastinum', 'cardiomegaly', 'airspace_opacity', 'lung_lesion', 'edema', 'consolidation', 'pneumonia', 'atelectasis', 'pneumothorax', 'pleural_effusion', 'pleural_other', 'fracture', 'support_devices'] {type:\"string\"}\n",
        "\n",
        "predicted = pred_true_df['predicted_{}'.format(label)]\n",
        "true = pred_true_df['true_{}'.format(label)]\n",
        "\n",
        "# Ignore uncertain labels for calculating metrics\n",
        "certain_mask = (true != LabelValues.uncertain.value)\n",
        "true = true[certain_mask]\n",
        "predicted = predicted[certain_mask]\n",
        "# Use the same encodings as during training:\n",
        "# not_mentioned, negative -\u003e 0\n",
        "# positive -\u003e 1\n",
        "true = np.array([float(LabelValues.positive == v) for v in LabelValues])[true]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "\n",
        "# Plot the precision-recall curve\n",
        "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(\n",
        "    true, predicted)\n",
        "average_precision = sklearn.metrics.average_precision_score(true, predicted)\n",
        "\n",
        "pr_axis = axes[0]\n",
        "pr_axis.plot(recall, precision)\n",
        "pr_axis.set_aspect('equal')\n",
        "pr_axis.set_xlim(0, 1)\n",
        "pr_axis.set_ylim(0, 1)\n",
        "pr_axis.set_xlabel(r'Recall $\\left(\\frac{T_p}{T_p + F_n} \\right)$')\n",
        "pr_axis.set_ylabel(r'Precision $\\left(\\frac{T_p}{T_p + F_p} \\right)$')\n",
        "pr_axis.set_title('Precision-Recall, AP={:.2f}'.format(average_precision))\n",
        "pr_axis.grid(True)\n",
        "\n",
        "# Plot the ROC curve\n",
        "fpr, tpr, thresholds = sklearn.metrics.roc_curve(true, predicted)\n",
        "roc_auc = sklearn.metrics.roc_auc_score(true, predicted)\n",
        "\n",
        "roc_axis = axes[1]\n",
        "roc_axis.plot(fpr, tpr)\n",
        "roc_axis.set_aspect('equal')\n",
        "roc_axis.set_xlim(0, 1)\n",
        "roc_axis.set_ylim(0, 1)\n",
        "roc_axis.set_xlabel(\n",
        "    r'False positive rate $\\left(\\frac{F_p}{F_p + T_n} \\right)$')\n",
        "roc_axis.set_ylabel(r'True positive rate $\\left(\\frac{T_p}{T_p + F_n} \\right)$')\n",
        "roc_axis.set_title('ROC, AUC={:.2f}'.format(roc_auc))\n",
        "roc_axis.grid(True)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0dXMQQ1hjSQQ",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Project Ideas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6BWSMKIRjYgy",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "Ready to go off and explore MIMIC CXR on your own? Here are some ideas for\n",
        "projects your team could work on.\n",
        "\n",
        "1.  Multiple models are provided as pretrained models. How will your team choose\n",
        "    the best one? Could you combine them into an ensemble method?\n",
        "1.  The pretrained models handle \"uncertain\" labels differently. How does this\n",
        "    choice impact the model's predictions? Are the results what you would\n",
        "    expect? Can we use these results to learn something about how each of the\n",
        "    different labels were generated?\n",
        "1.  Most imaging studies have multiple frontal and lateral images. How can you\n",
        "    combine the predictions from each image into a prediction for each study?\n",
        "    Would a logistic-linear model work? What about using the maximum prediction\n",
        "    in each category?\n",
        "1.  Investigate the images that are taken from a view other than frontal or\n",
        "    lateral. What do these images look like? Could you design and/or train a\n",
        "    model that uses these images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "15WI0XNhDRc7",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cJcDKdkejWrk",
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "That's it! You've trained a Convolutional Neural Network to classify chest X-ray\n",
        "images.\n",
        "\n",
        "We've just scratched the surface of machine learning on GCP. The\n",
        "[Cloud AI Platform](https://cloud.google.com/ai-platform/) has a full suite of\n",
        "tools for research to production ML development including preprocessing\n",
        "pipelines, Jupyter notebooks, distributed training, and model serving."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/brain/python/client:tpu_hw_notebook",
        "kind": "private"
      },
      "name": "mimic_cxr_train.ipynb",
      "provenance": [
        {
          "file_id": "1wJF3IQEczrGvX_ysdcSNYphQXUNN1UvN",
          "timestamp": 1558983612786
        },
        {
          "file_id": "/piper/depot/google3/third_party/cloud/healthcare/datathon/mimic_cxr/mimic_cxr_train.ipynb?workspaceId=reidhayes:imaging-datathon-int::citc",
          "timestamp": 1558727646133
        },
        {
          "file_id": "1gN9P9owTsXPMLlcP3lonYLGI4ERQqlk1",
          "timestamp": 1557769834852
        }
      ],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
