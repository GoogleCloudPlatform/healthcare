{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BZAzCqbzkS7O"
      },
      "source": [
        "# **Introduction**\n",
        "This tutorial shows how to use Google Cloud Platform technologies to work with structured healthcare data to build a predictive model.\n",
        "\n",
        "**Synthea**\n",
        "\n",
        "[Synthea](https://github.com/synthetichealth/synthea) is a data generator that simulates the lives of patients based on several medical modules. Each module models a different medical condition based on some real world statistics. Each patient in the Synthea dataset dies either due to medical reasons or non-medical random events not modeled by the generator.\n",
        "\n",
        "**Problem definition**\n",
        "\n",
        "Given the patient records generated by Synthea, predict the probability of the patient dying due to medical reasons.\n",
        "\n",
        "**Overview**\n",
        "\n",
        "Setup: Authentication, importing libraries, and project and dataset naming. NOTE: You must execute these commands every time you reconnect to Colab.\n",
        "\n",
        "Data generation: Download Synthea and run it, and then export the data to BigQuery.\n",
        "\n",
        "Feature extraction: Pivot a vertical table, join with horizontal, simplify.\n",
        "\n",
        "Model: Identify columns with sufficient data, train the model using BigQuery ML.\n",
        "\n",
        "Explore: Check model weights and most important features.\n",
        "\n",
        "Model with Tables: Train a neural net using AutoML Tables and compare the results with the BigQuery ML model.\n",
        "\n",
        "**Flow/reconnecting**\n",
        "\n",
        "Whenever you restart or reconnect to this notebook, run the steps in Setup again.\n",
        "The remaining steps need to be performed in order, but they do not need\n",
        "to be repeated after you complete them once.\n",
        "\n",
        "**Prerequisites**\n",
        "\n",
        "This notebook is accompanied by another notebook that generates the Synthea dataset and imports it into BigQuery. Run that notebook before running this one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0r6eqehZzfup"
      },
      "source": [
        "## Requirements\n",
        "To run this tutorial, you will need a GCP project with a billing account.\n",
        "\n",
        "## Costs\n",
        "There is a small cost associated with importing the dataset and storing it in BigQuery.\n",
        "However, if you run the AutoML Tables step at the end of the tutorial, the costs can reach up to $20 per hour of model training.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, sign into your Google account to access Google Cloud\n",
        "Platform (GCP).\n",
        "\n",
        "You will also import some standard Python data analysis packages that\n",
        "you'll use later to extract features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aGj0Aldmzse3"
      },
      "source": [
        "**Authentication**: Run the following commands, click on the link that displays, and follow the instructions to authenticate. Scroll to the results box to the left to see where to paste the key you will copy from the browser.\n",
        "\n",
        "NOTE: You will need to repeat this step each time you reconnect to the notebook server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kpbzGWDwGvLZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "credentials = auth._check_adc()\n",
        "print(credentials)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K24XIwhP826n"
      },
      "source": [
        "**Library imports**:\n",
        "\n",
        "NOTE: You will need to repeat this step each time you reconnect to the notebook server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rfvV5UDsGGbO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KI5qIIoh9Qrz"
      },
      "source": [
        "**Setup**:\n",
        "\n",
        "Enter the name of your GCP project. The dataset name, output table, and model names are supplied for you. Use the same GCP project and dataset that you used when importing Synthea data in the previous notebook.\n",
        "\n",
        "NOTE: You will need to repeat this step each time you reconnect to the notebook server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sOm2d4mj9VDS"
      },
      "outputs": [],
      "source": [
        "project = \"\" #@param {type:\"string\"}\n",
        "if not project:\n",
        "  raise Exception(\"Project is empty.\")\n",
        "!gcloud config set project $project\n",
        "\n",
        "\n",
        "dataset = \"SYNMASS_2k\" #@param {type:\"string\"}\n",
        "\n",
        "output_table = \"ml_ready_table_1\" #@param {type:\"string\"}\n",
        "ml_ready_table_name = \"{}.{}.{}\".format(project, dataset, output_table)\n",
        "\n",
        "model_name = \"mortality_model_1\" #@param {type:\"string\"}\n",
        "full_model_name = \"{}.{}\".format(dataset, model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MAbFpJl-cWtw"
      },
      "source": [
        "# Feature extraction\n",
        "Like many healthcare datasets, the Synthea dataset contains both \"vertical/longitudinal\" and \"horizontal\" tables.\n",
        "\n",
        "**Horizontal tables**\n",
        "\n",
        "Horizontal tables contain one row per patient. Each column provides a piece of information about that patient. A good example of this is the *patients* table where each column provides a demographic feature of the patient such as gender, race, and so forth. A more technical term for horizontal tables is first normal form (1NF).\n",
        "\n",
        "**Vertical or longitudinal tables**\n",
        "\n",
        "Vertical tables are usually used to store logitudinal measurements and observations. Unlike horizontal tables, each patient can have multiple rows representing different measurements/observations at different times. An example is the *observations* table where each row can contain the result of a specific lab test. The *description* column provides the name of the lab test and the *value* column determines the outcome of the test. A more technical term for vertical tables is entity–attribute–value model.\n",
        "\n",
        "**ML-ready table**\n",
        "\n",
        "You cannot directly train the regression model on the dataset as-is in BigQuery. Instead, you will have a simple horizontal table where each training example corresponds to a single row containing all of the data for one patient and each column corresponds to a feature. The name for this type of table is an *ML-ready table*.\n",
        "\n",
        "**Transformation**\n",
        "\n",
        "The following code extracts data from the source vertical and horizontal tables, joins them based on patient ID, and creates a single ML-ready table.\n",
        "To extract features from the vertical tables, a query is built that groups the rows by their description and aggregates the corresponding values into an array sorted by time. For each description that occurs at least 100 times, a new column is added to the ML-ready table, which will be a feature for the model. The name of the column comes from the description and the value of the column is the last value in the aggregated array. Therefore, for  each type of measurement, the last measurement is used as the value of the corresponding feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tE5ij3CZIvUU"
      },
      "outputs": [],
      "source": [
        "_MIN_DESCRIPTION_OCCURENCES = 100\n",
        "\n",
        "def GetFullTableName(name):\n",
        "  \"\"\"Returns the full BQ table name for the given short table name.\"\"\"\n",
        "\n",
        "  return \"{}.{}.{}\".format(project, dataset, name)\n",
        "\n",
        "\n",
        "def UpdateFieldNameToDesc(field_name_to_desc, prefix, descriptions_to_exclude,\n",
        "                          description):\n",
        "  \"\"\"Updates a given field name to description dictionary with the given values.\n",
        "\n",
        "  The description is converted to a valid BQ field name, and then the\n",
        "  dictionary is updated to map the field name to the description.\n",
        "\n",
        "  Args:\n",
        "    field_name_to_desc: The map that should be updated.\n",
        "    prefix: The prefix used for the normalized field name. This is required to\n",
        "      differentiate between same descriptions in different tables.\n",
        "    descriptions_to_exclude: A list of descriptions that should be excluded from\n",
        "      the map.\n",
        "    description: the description that should be added to the map.\n",
        "  \"\"\"\n",
        "  if description in descriptions_to_exclude:\n",
        "    return\n",
        "  pattern = re.compile(r\"[\\W_]+\")\n",
        "  field_name = pattern.sub(\" \", description)\n",
        "  field_name = field_name.replace(\" \", \"_\")\n",
        "  field_name = \"{}_{}\".format(prefix, field_name)\n",
        "  field_name_to_desc[field_name] = description\n",
        "\n",
        "\n",
        "def BuildFieldNameToDesc(prefix, table, typ, descriptions_to_exclude):\n",
        "  \"\"\"Reads a vertical table and returns a map of field name to description.\n",
        "\n",
        "  The description value of the rows determines the column name of the\n",
        "  ML-ready table. We extract all the unique descriptions and transform them\n",
        "  to a valid name that can be used as BQ column names.\n",
        "\n",
        "  Args:\n",
        "    prefix: The prefix used for the normalized field names.\n",
        "    table: The name of the table for which the map is built.\n",
        "    typ: If this is set, the output is limited to the fields having this type.\n",
        "    descriptions_to_exclude: A list of descriptions that should be excluded from\n",
        "      the map.\n",
        "\n",
        "  Returns:\n",
        "    A map from normalized BQ field names to their corresponding description.\n",
        "  \"\"\"\n",
        "\n",
        "  type_constraint = \"\"\n",
        "  if typ is not None:\n",
        "    type_constraint = \" WHERE TYPE='{}' \".format(typ)\n",
        "  sql = \"\"\"\n",
        "    SELECT\n",
        "      DESCRIPTION as description,\n",
        "      count(*) as occurences\n",
        "    FROM `{}`{}\n",
        "    GROUP BY 1 ORDER BY 2 DESC\"\"\".format(table, type_constraint)\n",
        "\n",
        "  data = pd.read_gbq(query=sql, project_id=project, dialect=\"standard\")\n",
        "\n",
        "  # Filter the data to contain descriptions that have at least\n",
        "  # _MIN_DESCRIPTION_OCCURENCES occurences.\n",
        "  data = data[data[\"occurences\"] \u003e _MIN_DESCRIPTION_OCCURENCES]\n",
        "\n",
        "  field_name_to_desc = {}\n",
        "\n",
        "  def UpdateFn(description):\n",
        "    UpdateFieldNameToDesc(field_name_to_desc, prefix, descriptions_to_exclude,\n",
        "                          description)\n",
        "\n",
        "  data[\"description\"].apply(UpdateFn)\n",
        "  return field_name_to_desc\n",
        "\n",
        "\n",
        "def BuildQueryToHorizontalize(prefix, table, typ, descriptions_to_exclude):\n",
        "  \"\"\"Builds a query that horizontalizes the given table.\n",
        "\n",
        "  The description column determines the feature name and the value column\n",
        "  determines the value. In case of multiple values for a description, the last\n",
        "  value is used.\n",
        "\n",
        "  Args:\n",
        "    prefix: The prefix used for the normalized field names.\n",
        "    table: The name of the table for which the query is built.\n",
        "    typ: Type of the values, it can be either \"numeric\" or \"text\".\n",
        "    descriptions_to_exclude: descriptions that shouldn't be featurized.\n",
        "\n",
        "  Returns:\n",
        "    A sql query to horizontalize the table.\n",
        "  \"\"\"\n",
        "\n",
        "  field_name_to_desc = BuildFieldNameToDesc(prefix, table, typ,\n",
        "                                            descriptions_to_exclude)\n",
        "  columns_str = \"\"\n",
        "  for field_name, desc in field_name_to_desc.items():\n",
        "    if typ == \"numeric\":\n",
        "      columns_str += (\n",
        "          \", any_value(if(DESCRIPTION = \\\"{}\\\", CAST(values[OFFSET(0)] as \"\n",
        "          \"float64), NULL)) AS {}\\n\").format(desc, field_name)\n",
        "    else:\n",
        "      columns_str += (\", any_value(if(DESCRIPTION = \\\"{}\\\", values[OFFSET(0)], \"\n",
        "                      \"NULL)) AS {}\\n\").format(desc, field_name)\n",
        "\n",
        "  sql = \"\"\"\n",
        "  SELECT\n",
        "    PATIENT\n",
        "  {}\n",
        "  FROM (\n",
        "    SELECT\n",
        "      PATIENT,\n",
        "      DESCRIPTION,\n",
        "      ARRAY_AGG(VALUE order by DATE desc) as values\n",
        "    FROM `{}` group by 1,2) group by 1\"\"\".format(columns_str, table)\n",
        "  return sql\n",
        "\n",
        "\n",
        "def BuildQueryToHorizontalizeBinaryFeatures(prefix, table):\n",
        "  \"\"\"Builds a query to horizontalize the table.\n",
        "\n",
        "  If a patient has no row with a given description the corresponding feature\n",
        "  will have value 0, otherwise 1.\n",
        "\n",
        "  Args:\n",
        "    prefix: The prefix used for the normalized field names.\n",
        "    table: The name of the table for which the query is built.\n",
        "\n",
        "  Returns:\n",
        "    A sql query to horizontalize the table.\n",
        "  \"\"\"\n",
        "  descriptions_to_exclude = set()\n",
        "  field_name_to_desc = BuildFieldNameToDesc(\n",
        "      prefix, table, typ=None, descriptions_to_exclude=descriptions_to_exclude)\n",
        "  columns_str = \"\"\n",
        "  for field_name, desc in field_name_to_desc.items():\n",
        "    columns_str += \", sum(if(DESCRIPTION = \\\"{}\\\", 1, 0)) AS {}\\n\".format(\n",
        "        desc, field_name)\n",
        "\n",
        "  sql = \"\"\"\n",
        "  SELECT\n",
        "    PATIENT\n",
        "  {}\n",
        "  FROM (\n",
        "    SELECT\n",
        "      PATIENT,\n",
        "      DESCRIPTION\n",
        "    FROM `{}` group by 1,2) group by 1\"\"\".format(columns_str, table)\n",
        "  return sql\n",
        "\n",
        "\n",
        "def BuildQueryToExtractHorizontalFeatures(table, columns):\n",
        "  \"\"\"Builds a query to extract a given set of features from a horizontal table.\"\"\"\n",
        "\n",
        "  features_str = \"\"\n",
        "  for col in columns:\n",
        "    features_str += \", {}\".format(col)\n",
        "\n",
        "  sql = \"\"\"\n",
        "  SELECT\n",
        "    Id as PATIENT\n",
        "  {}\n",
        "  FROM `{}`\"\"\".format(features_str, table)\n",
        "  return sql\n",
        "\n",
        "\n",
        "def BuildLabelQuery(table):\n",
        "  \"\"\"Returns the query to build a table with patient id and label columns.\"\"\"\n",
        "\n",
        "  sql = \"\"\"\n",
        "  SELECT\n",
        "    PATIENT,\n",
        "    sum(if(DESCRIPTION=\"Death Certification\", 1, 0)) as LABEL\n",
        "  FROM `{}` group by 1\"\"\".format(table)\n",
        "  return sql\n",
        "\n",
        "\n",
        "# Build queries to extract features from patients, observations, and conditions\n",
        "# tables.\n",
        "\n",
        "demographics = BuildQueryToExtractHorizontalFeatures(\n",
        "    table=GetFullTableName(\"patients\"),\n",
        "    columns=[\"ethnicity\", \"gender\", \"city\", \"race\"])\n",
        "\n",
        "# Exclude the rows that contains the cause of death, otherwise it would be\n",
        "# cheating ;).\"\".\n",
        "numeric_observations = BuildQueryToHorizontalize(\n",
        "    prefix=\"obs\",\n",
        "    table=GetFullTableName(\"observations\"),\n",
        "    typ=\"numeric\",\n",
        "    descriptions_to_exclude=set(\n",
        "        [\"Cause of Death [US Standard Certificate of Death]\"]))\n",
        "\n",
        "text_observations = BuildQueryToHorizontalize(\n",
        "    prefix=\"obs\",\n",
        "    table=GetFullTableName(\"observations\"),\n",
        "    typ=\"text\",\n",
        "    descriptions_to_exclude=set(\n",
        "        [\"Cause of Death [US Standard Certificate of Death]\"]))\n",
        "\n",
        "# Conditions are modeled as binary features, the corresponding column is\n",
        "# true if and only if there is a row in conditions table with matching\n",
        "# description.\n",
        "conditions = BuildQueryToHorizontalizeBinaryFeatures(\n",
        "    prefix=\"cond\", table=GetFullTableName(\"conditions\"))\n",
        "\n",
        "# Build the query for the label table.\n",
        "label = BuildLabelQuery(table=GetFullTableName(\"encounters\"))\n",
        "\n",
        "# Build the main query that uses subqueries to extract the label, and\n",
        "# verticalize observations and conditions tables. The result of subqueries\n",
        "# are joined based on patient ID.\n",
        "sql_query = \"\"\"\n",
        "  SELECT * FROM ({})\n",
        "  left join ({}) using (PATIENT)\n",
        "  left join ({}) using (PATIENT)\n",
        "  left join ({}) using (PATIENT)\n",
        "  left join ({}) using (PATIENT)\"\"\".format(numeric_observations,\n",
        "                                           text_observations, conditions,\n",
        "                                           demographics, label)\n",
        "\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "\n",
        "# Set the destination table\n",
        "table_name = ml_ready_table_name.split(\".\")[-1]\n",
        "\n",
        "bq_client = bigquery.Client(project=project)\n",
        "\n",
        "table_ref = bq_client.dataset(dataset).table(table_name)\n",
        "job_config.destination = table_ref\n",
        "job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
        "\n",
        "# Start the query, passing in the extra configuration.\n",
        "query_job = bq_client.query(\n",
        "    sql_query,\n",
        "    # Location must match that of the dataset(s) referenced in the query\n",
        "    # and of the destination table.\n",
        "    location=\"US\",\n",
        "    job_config=job_config)  # API request - starts the query\n",
        "\n",
        "query_job.result()  # Waits for the query to finish\n",
        "print(\"Query results loaded to table {}\".format(table_ref.path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cp7sA97EZ8J8"
      },
      "source": [
        "#Model training\n",
        "\n",
        "Now that the data is transformed into the ML-ready table, you are ready to train a model. At this point, you have several options to train a model including BigQuery ML, AutoML tables, and Cloud Machine Learning Engine. This tutorial focuses on the simplest and quickest tool, BigQuery ML (BQML), to train a linear logistic regression model to predict the probablity of death due to medical reasons.\n",
        "\n",
        " BQML automatically applies the\n",
        "required\n",
        "[transformations](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create#input_variable_transformations)\n",
        "depending on each variable's data type. For example, `STRING`s are transformed\n",
        "into [one-hot](https://en.wikipedia.org/wiki/One-hot) vectors, and `TIMESTAMP`s\n",
        "are\n",
        "[standardized](https://en.wikipedia.org/wiki/Feature_scaling#Standardization).\n",
        "\n",
        "BQML also let you apply regularization to help with the generalization error. In the example here, because you are using features that all resulted from verticalization of the observations and conditions table, you will use relatively large l1 regularization coefficients to avoid overfitting.\n",
        "\n",
        "This step takes ~5 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wONZrwNN-mbA"
      },
      "source": [
        "Set up the context for `bigquery.magics` which you will use in the following sections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "WDj5X5CkzMGU"
      },
      "outputs": [],
      "source": [
        "# Set the default project for running queries\n",
        "bigquery.magics.context.project = project \n",
        "\n",
        "# Set up the substitution preprocessing injection\n",
        "# This is used to be able to configure bigquery magic with ml_ready_table_name\n",
        "# parameter\n",
        "sub_dict = dict()\n",
        "sub_dict[\"model_name\"] = \"{}.{}\".format(dataset, model_name)\n",
        "sub_dict[\"ml_ready_table_name\"] = ml_ready_table_name\n",
        "if globals().get('custom_run_query') is None:\n",
        "  original_run_query = bigquery.magics._run_query\n",
        "\n",
        "  def custom_run_query(client, query, job_config=None):\n",
        "    query = query.format(**sub_dict)\n",
        "    return original_run_query(client, query, job_config)\n",
        "\n",
        "  bigquery.magics._run_query = custom_run_query\n",
        "print('done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tPhZr7mV-6Al"
      },
      "source": [
        "Next, run the following commands to perform the actual model training and evaluation using BigQuery ML.\n",
        "\n",
        "NOTE: If the following command fails with a permission error, check your Cloud IAM settings and make sure that the default Compute Engine service account (PROJECT_NUMBER-compute@developer.gserviceaccount.com) has a role with BigQuery model creation permissions, such as roles/bigquery.dataEditor or BigQuery Data Editor. This happens only if you have intentionally changed the role for the Compute Engine default service account. The default role for this account has BigQuery Data Editor, which has all the required permissions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lEG0ipyzkd2X"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "# BigQuery ML create model statement:\n",
        "CREATE OR REPLACE MODEL `{model_name}`\n",
        "OPTIONS(\n",
        "  # Use logistic_reg for discrete predictions (classification) and linear_reg\n",
        "  # for continuous predictions (forecasting).\n",
        "  model_type = 'logistic_reg',\n",
        "  early_stop = False,\n",
        "  max_iterations = 25,\n",
        "  l1_reg = 2,\n",
        "  # Identify the column to use as the label.\n",
        "  input_label_cols = [\"LABEL\"]\n",
        ")\n",
        "AS\n",
        "SELECT\n",
        "  *\n",
        "FROM `{ml_ready_table_name}`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V_Y5wY23-awA"
      },
      "source": [
        "#Exploring the results\n",
        "\n",
        "To see the training metrics, go to the BigQuery dashboard in Cloud Console and select the project that you are running this tutorial in. You can then find your model under the dataset you used in the model_name.\n",
        "\n",
        "BigQuery shows you useful plots on training and evaluation tabs. On the training tab, the training and validation loss are plotted as a function of iterations. You can also see the learning rate used at each iteration.\n",
        "\n",
        "The evaluation tab also provides useful accuracy metrics like F1 score, Log loss and ROC AUC. You should see an AUC of around 0.8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "shEnKkfcjsKR"
      },
      "source": [
        "![alt text](https://github.com/GoogleCloudPlatform/healthcare/blob/master/ml_solutions/resources/bqml_results.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j3lOgeTbAH_5"
      },
      "source": [
        "**Inspecting the weights of the different features**\n",
        "\n",
        "Because you converted all of the data in the conditions and observations tables to features, you might ask whether all of these features are required to train a model. Because Bigquery ML trains linear models, you can answer this question by inspecting the weights learned for the features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lzhG4kWZD2yj"
      },
      "source": [
        "Run the following query to view the top 10 categorical features having the largest weight variance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hizpDIYJAyBC"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "SELECT\n",
        "  processed_input,\n",
        "  STDDEV(cws.weight) as stddev_w,\n",
        "  max(cws.weight) as max_w,\n",
        "  min(cws.weight) as min_w\n",
        "from (\n",
        "  SELECT processed_input, cws\n",
        "  FROM\n",
        "      ML.WEIGHTS(MODEL `{model_name}`)\n",
        "    cross join unnest(category_weights) as cws\n",
        ")\n",
        "group by 1\n",
        "order by 2 desc limit 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8-n6JpLr_r5i"
      },
      "source": [
        "Run the following query to view the top 10 categorical features by maximum absolute weight value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2DlznAIZD90W"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "SELECT processed_input, max(abs(weight))\n",
        "FROM ML.WEIGHTS(MODEL `{model_name}`)\n",
        "group by 1\n",
        "order by 2 desc limit 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "naFJFpRE_-yx"
      },
      "source": [
        "#AutoML Tables\n",
        "If you succesfully trained a model using BigQuery ML, it means that your \"ml_ready_table\" is in good shape. You can now use AutoML Tables to train a neural net. This model should perform better than the logistic regression model that you trained using BigQuery ML."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jqujsxSdACMU"
      },
      "source": [
        "Complete the following steps to enable the AutoML Tables API and view the UI:\n",
        "\n",
        "1. Go to the Google Cloud Console.\n",
        "\n",
        "2. Select the project you are using for this demo from the project dropdown at the top of the window.\n",
        "\n",
        "3. Go to the main menu by selecting the \"hamburger\" at the upper left of the window, and scroll down near the bottom of the menu to the Artificial Intelligence section and select Tables.\n",
        "\n",
        "4. If this is your first time using Tables on this project, you will be asked to enable the API. This will take several minutes.\n",
        "\n",
        "5. After the API is enabled, you will be taken to the dataset screen. Select New Dataset. Give the dataset a name and select Create Dataset.\n",
        "\n",
        "6. You will then be taken to the AutoML Tables GUI Import tab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UBHs_jedAQj2"
      },
      "source": [
        "#Import \"ml_ready_table\" to AutoML Tables\n",
        "\n",
        "To import the \"ml_ready_table\" table to Tables, complete the following steps:\n",
        "\n",
        "1. Make sure that **Import data from BigQuery** is selected.\n",
        "\n",
        "2. Enter your **project ID**, **dataset ID**, and **table ID** for this tutorial. Pay close attention to the use of underscores (_) and dashes (-) when entering these values.\n",
        "\n",
        "NOTE: These are the \"project\", \"dataset\", and \"output table\" that you entered at the beginning of this tutorial.\n",
        "\n",
        "3. Select **IMPORT**.\n",
        "\n",
        "If you get an error, carefully check your project, dataset and table names.\n",
        "Importing will take several minutes. You will receive an email when importing is complete.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ETpoBo0NAXez"
      },
      "source": [
        "#TRAIN\n",
        "\n",
        "After you finish with the import, you will end up on the **TRAIN** page.\n",
        "\n",
        "Here, you need to use the dropdown to pick the **Target Column**, which for your table is called **LABEL**.\n",
        "\n",
        "On this tab, you can review statistics about your inputs including the distribution of different values, the percentage of missing and invalid values.\n",
        "\n",
        "When you are done checking out the data statistics, click on the **TRAIN MODEL** button. Here you need to assign a training budget. For this tutorial, two hours of training is adequate, but three hours might give you better results. (Training costs approximately $20/hour).\n",
        "\n",
        "Then hit the **TRAIN MODEL** button.\n",
        "\n",
        "Training will take a little more than the budgeted training time. You will receive an email when training is complete.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xGLPL1QHAasE"
      },
      "source": [
        "#EVALUATE\n",
        "\n",
        "After training is done (again, you will receive an email), follow the link on the email to get to the **EVALUATE** tab. Successful evaluation results will look something like the below screenshot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pEce5z_mj8sV"
      },
      "source": [
        "![alt text](https://github.com/GoogleCloudPlatform/healthcare/blob/master/ml_solutions/resources/automl_results.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SCydeExfAi21"
      },
      "source": [
        "#CLEAN UP\n",
        "\n",
        "Because the BigQuery tables and models developed by this tutorial are in your GCP project, you will be billed for storage on an ongoing basis. After you have finished exploring these assets, you will want to delete them to avoid recurring charges.  You can do this by deleting the resources individually or by deleting the entire project which will delete all of the underlying resources."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "synthea_bqml_automl.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
