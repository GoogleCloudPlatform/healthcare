# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Sample template to create a single data-hosting project with audit logs saved
# to the same project. Deployed project has GCS buckets, BigQuery datasets,
# logging to GCS and BigQuery, and logs-based
# metrics for unexpected access.
#
# Usage:
# 1) Replace all fields with relevant values in (a copy of) this file.
# 2) Run gcloud init
# 3) run ./create_project.py --project_yaml=${NEW_PROJECT_YAML?}
# 4) When prompted, follow the steps to set up a Stackdriver account.

overall:
  organization_id: '12345678'           # Replace this with your Organization ID, or remove to setup without an organization.
  folder_id: '98765321'                 # Optional. Replace this with an existing folder within the organization.
  billing_account: 000000-000000-000000 # Replace this with your Billing Account.

projects:
- project_id: my-project
  owners_group: my-project-owners@my-domain.com      # Replace this with the owners group for this project.
  # editors_group: my-project-editors@mydomain.com   # If an editors group is required, uncomment this field and set the value. (Not recommended)
  auditors_group: some-auditors-group@my-domain.com  # Replace this with your auditors group.
  data_readwrite_groups:
  - some-readwrite-group@my-domain.com
  data_readonly_groups:
  - some-readonly-group@my-domain.com
  - another-readonly-group@googlegroups.com
  # BigQuery (non-auditlogs) datasets.
  bigquery_datasets:
  - name: us_data
    location: US
  - name: euro_data
    location: EU
  # Data buckets.
  data_buckets:
  - name_suffix: -nlp-bucket
    location: US-CENTRAL1
    storage_class: REGIONAL
    # If expected_users is provided, create an unexpected access metric for
    # accesses from other users.
    expected_users:
    - auth_user_1@mydomain.com
    - auth_user_2@mydomain.com
  - name_suffix: -other-bucket
    location: US-EAST1
    storage_class: REGIONAL
  - name_suffix: -euro-bucket
    location: EUROPE-WEST1
    storage_class: REGIONAL
    expected_users:
    - auth_user_3@mydomain.com
  # Pubsub topic.
  pubsub:
    topic: test-topic
    subscription: test-subscription
    publisher_account: cloud-healthcare-eng@system.gserviceaccount.com
    ack_deadline_sec: 100
  # If alert email is present, create StackDriver alerts.
  stackdriver_alert_email: some-alerts-group@my-domain.com
  # These audit logs will be saved to this project.
  dataproc:
  - name: dataproc-lab
    zone: us-central1-c
    region: global
    workernum: 2
    masternum: 1
    machine_type: n1-standard-1
    tags:
    - name: tag1
    init_scripts:
    - name: script1.sh
    - name: script2.sh
    imageVersion: '1.3'
    firewall_rules:
    - name: allow-datalab
      allowed:
      - IPProtocol: tcp
        ports: ['8080']
      - IPProtocol: tcp
        ports: ['8088']
      - IPProtocol: tcp
        ports: ['9870']
      sourceRanges: [0.0.0.0/0]
      targetTags: [<tag1>,<tag2>]
  enabled_apis:
  - bigquery-json.googleapis.com  # BigQuery
  - compute.googleapis.com        # Google Compute Engine
  - dataproc.googleapis.com       # Dataproc apis
  audit_logs:
    logs_gcs_bucket:
      # Local logs GCS bucket will have the name '{PROJECT_ID}-logs'.
      location: US
      storage_class: MULTI_REGIONAL
      ttl_days: 365
    logs_bigquery_dataset:
      # Local logs dataset will have the name 'audit_logs'.
      location: US

