{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VbkyG_K1LSnC"
      },
      "source": [
        "Copyright 2018 Google Inc.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EHcImnV1yZ5z"
      },
      "source": [
        "**This tutorial is for educational purposes purposes only and is not intended for use in clinical diagnosis or clinical decision-making or for any other clinical use.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q8ZHSBelKd8V"
      },
      "source": [
        "# Training/Inference on Breast Density Classification Model on AutoML Vision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XCClehsPK2Jq"
      },
      "source": [
        "The goal of this tutorial is to train, deploy and run inference on a breast density classification model. Breast density is thought to be a factor for an increase in the risk for breast cancer. This will emphasize using the [Cloud Healthcare API](https://cloud.google.com/healthcare/) in order to store, retreive and transcode medical images (in DICOM format) in a managed and scalable way. This tutorial will focus on using [Cloud AutoML Vision](https://cloud.google.com/vision/automl/docs/beginners-guide) to scalably train and serve the model. \n",
        "\n",
        "**Note: This is the AutoML version of the Cloud ML Engine Codelab found [here](./breast_density_cloud_ml.ipynb).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u1OUa9pCPtqu"
      },
      "source": [
        "## Requirements\n",
        "- A Google Cloud project.\n",
        "- Project has [Cloud Healthcare API](https://cloud.google.com/healthcare/docs/quickstart) enabled.\n",
        "- Project has [Cloud AutoML API ](https://cloud.google.com/vision/automl/docs/quickstart) enabled.\n",
        "- Project has [Cloud Build API](https://cloud.google.com/cloud-build/docs/quickstart-docker) enabled.\n",
        "- Project has [Kubernetes engine API](https://console.developers.google.com/apis/api/container.googleapis.com/overview?project=) enabled.\n",
        "- Project has [Cloud Resource Manager API](https://console.cloud.google.com/cloud-resource-manager) enabled."
      ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## HCLS Imaging ML Toolkit\n",
      "We will also need to install the hcls_imaging_ml_toolkit package found [here](./toolkit). This toolkit helps make working with DICOM objects and the Cloud Healthcare API easier."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "!pip3 install git+https://github.com/GoogleCloudPlatform/healthcare.git#subdirectory=imaging/ml/toolkit"
     ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qL-vcMmZRITm"
      },
      "source": [
        "## Input Dataset\n",
        "\n",
        "The dataset that will be used for training is the [TCIA CBIS-DDSM](https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM) dataset. This dataset contains ~2500 mammography images in DICOM format. Each image is given a [BI-RADS breast density ](https://breast-cancer.ca/densitbi-rads/) score from 1 to 4. In this tutorial, we will build a binary classifier that distinguishes between breast density \"2\" (*scattered density*) and \"3\" (*heterogeneously dense*). These are the two most common and variably assigned scores. In the literature, this is said to be [particularly difficult for radiologists to consistently distinguish](https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.12683)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NSr5StGBZkYd"
      },
      "outputs": [],
      "source": [
        "project_id = \"MY_PROJECT\" # @param\n",
        "location = \"us-central1\"\n",
        "dataset_id = \"MY_DATASET\" # @param\n",
        "dicom_store_id = \"MY_DICOM_STORE\" # @param\n",
        "\n",
        "# Input data used by AutoML must be in a bucket with the following format.\n",
        "automl_bucket_name = \"gs://\" + project_id + \"-vcm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UW3Y6Pd6d1Ey"
      },
      "outputs": [],
      "source": [
        "%%bash -s {project_id} {location} {automl_bucket_name}\n",
        "# Create bucket.\n",
        "gsutil -q mb -c regional -l $2 $3\n",
        "\n",
        "# Allow Cloud Healthcare API to write to bucket.\n",
        "PROJECT_NUMBER=`gcloud projects describe $1 | grep projectNumber | sed 's/[^0-9]//g'`\n",
        "SERVICE_ACCOUNT=\"service-${PROJECT_NUMBER}@gcp-sa-healthcare.iam.gserviceaccount.com\"\n",
        "COMPUTE_ENGINE_SERVICE_ACCOUNT=\"${PROJECT_NUMBER}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "gsutil -q iam ch serviceAccount:${SERVICE_ACCOUNT}:objectAdmin $3\n",
        "gsutil -q iam ch serviceAccount:${COMPUTE_ENGINE_SERVICE_ACCOUNT}:objectAdmin $3\n",
        "gcloud projects add-iam-policy-binding $1 --member=serviceAccount:${SERVICE_ACCOUNT} --role=roles/pubsub.publisher\n",
        "gcloud projects add-iam-policy-binding $1 --member=serviceAccount:${COMPUTE_ENGINE_SERVICE_ACCOUNT} --role roles/pubsub.admin\n",
        "# Allow compute service account to create datasets and dicomStores.\n",
        "gcloud projects add-iam-policy-binding $1 --member=serviceAccount:${COMPUTE_ENGINE_SERVICE_ACCOUNT} --role roles/healthcare.dicomStoreAdmin\n",
        "gcloud projects add-iam-policy-binding $1 --member=serviceAccount:${COMPUTE_ENGINE_SERVICE_ACCOUNT} --role roles/healthcare.datasetAdmin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vgBA16ptbacM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import google.auth\n",
        "from google.auth.transport.requests import AuthorizedSession\n",
        "from hcls_imaging_ml_toolkit import dicom_path\n",
        "\n",
        "credentials, project = google.auth.default()\n",
        "authed_session = AuthorizedSession(credentials)\n",
        "# Path to Cloud Healthcare API.\n",
        "HEALTHCARE_API_URL = 'https://healthcare.googleapis.com/v1'\n",
        "\n",
        "# Create Cloud Healthcare API dataset.\n",
        "path = os.path.join(HEALTHCARE_API_URL, 'projects', project_id, 'locations', location, 'datasets?dataset_id=' + dataset_id)\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "resp = authed_session.post(path, headers=headers)\n",
        "\n",
        "assert resp.status_code == 200, 'error creating Dataset, code: {0}, response: {1}'.format(resp.status_code, resp.text)\n",
        "print('Full response:\\n{0}'.format(resp.text))\n",
        "\n",
        "# Create Cloud Healthcare API DICOM store.\n",
        "path = os.path.join(HEALTHCARE_API_URL, 'projects', project_id, 'locations', location, 'datasets', dataset_id, 'dicomStores?dicom_store_id=' + dicom_store_id)\n",
        "resp = authed_session.post(path, headers=headers)\n",
        "assert resp.status_code == 200, 'error creating DICOM store, code: {0}, response: {1}'.format(resp.status_code, resp.text)\n",
        "print('Full response:\\n{0}'.format(resp.text))\n"
        "dicom_store_path = dicom_path.Path(project_id, location, dataset_id, dicom_store_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HKsCfbXorosM"
      },
      "source": [
        "Next, we are going to transfer the DICOM instances to the Cloud Healthcare API.\n",
        "\n",
        "Note: We are transfering \u003e100GB of data so this will take some time to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "shhPzNFArpHH"
      },
      "outputs": [],
      "source": [
        "# Store DICOM instances in Cloud Healthcare API.\n",
        "path = \"https://healthcare.googleapis.com/v1/{}:import".format(dicom_store_path)\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "body = { \n",
        "      'gcsSource': {\n",
        "        'uri': 'gs://gcs-public-data--healthcare-tcia-cbis-ddsm/dicom/**'\n",
        "      }\n",
        "}\n",
        "resp = authed_session.post(path, headers=headers, json=body)\n",
        "assert resp.status_code == 200, 'error creating Dataset, code: {0}, response: {1}'.format(resp.status_code, resp.text)\n",
        "print('Full response:\\n{0}'.format(resp.text))\n",
        "response = json.loads(resp.text)\n",
        "operation_name = response['name']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "biHU0WoLjgYs"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def wait_for_operation_completion(path, timeout, sleep_time=30): \n",
        "  success = False\n",
        "  while time.time() \u003c timeout:\n",
        "    print('Waiting for operation completion...')\n",
        "    resp = authed_session.get(path)\n",
        "    assert resp.status_code == 200, 'error polling for Operation results, code: {0}, response: {1}'.format(resp.status_code, resp.text)\n",
        "    response = json.loads(resp.text)\n",
        "    if 'done' in response:\n",
        "      if response['done'] == True and 'error' not in response:\n",
        "        success = True;\n",
        "      break\n",
        "    time.sleep(sleep_time)\n",
        "\n",
        "  print('Full response:\\n{0}'.format(resp.text))      \n",
        "  assert success, \"operation did not complete successfully in time limit\"\n",
        "  print('Success!')\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "B7pNV8uYyYmy"
      },
      "outputs": [],
      "source": [
        "path = os.path.join(HEALTHCARE_API_URL, operation_name)\n",
        "timeout = time.time() + 40*60 # Wait up to 40 minutes.\n",
        "_ = wait_for_operation_completion(path, timeout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mjFm5w356Z6t"
      },
      "source": [
        "### Explore the Cloud Healthcare DICOM dataset (optional)\n",
        "\n",
        "This is an optional section to explore the Cloud Healthcare DICOM dataset. In the following code, we simply just list the studies that we have loaded into the Cloud Healthcare API. You can modify the *num_of_studies_to_print* parameter to print as many studies as desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RUjYgoym7MZN"
      },
      "outputs": [],
      "source": [
        "num_of_studies_to_print = 2 # @param\n",
        "\n",
        "\n",
        "path = os.path.join(HEALTHCARE_API_URL, dicom_store_path.dicomweb_path_str, 'studies')\n",
        "resp = authed_session.get(path)\n",
        "assert resp.status_code == 200, 'error querying Dataset, code: {0}, response: {1}'.format(resp.status_code, resp.text)\n",
        "response = json.loads(resp.text)\n",
        "\n",
        "print(json.dumps(response[:num_of_studies_to_print], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qpkeGeAsb-ec"
      },
      "source": [
        "## Convert DICOM to JPEG\n",
        "\n",
        "The ML model that we will build requires that the dataset be in JPEG. We will leverage the Cloud Healthcare API to transcode DICOM to JPEG.\n",
        "\n",
        "First we will create a [Google Cloud Storage](https://cloud.google.com/storage/) bucket to hold the output JPEG files. Next, we will use the ExportDicomData API to transform the DICOMs to JPEGs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sXB41cvKeLZj"
      },
      "outputs": [],
      "source": [
        "# Folder to store input images for AutoML Vision.\n",
        "jpeg_folder = automl_bucket_name + \"/images/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aIfKnxqHjE9A"
      },
      "source": [
        "Next we will convert the DICOMs to JPEGs using the [ExportDicomData](https://cloud.google.com/sdk/gcloud/reference/beta/healthcare/dicom-stores/export/gcs). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2Ic_OPeVjM5i"
      },
      "outputs": [],
      "source": [
        "%%bash -s {jpeg_folder} {project_id} {location} {dataset_id} {dicom_store_id}\n",
        "gcloud beta healthcare --project $2  dicom-stores export gcs $5 --location=$3 --dataset=$4 --mime-type=\"image/jpeg; transfer-syntax=1.2.840.10008.1.2.4.50\" --gcs-uri-prefix=$1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pVhp8IdVkVH3"
      },
      "source": [
        "Meanwhile, you should be able to observe the JPEG images being added to your Google Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "liblamUVyWZw"
      },
      "source": [
        "Next, we will join the training data stored in Google Cloud Storage with the labels in the TCIA website. The output of this step is a [CSV file that is input to AutoML](https://cloud.google.com/vision/automl/docs/prepare). This CSV contains a list of pairs of (IMAGE_PATH, LABEL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DgDI4Esw8bXT"
      },
      "outputs": [],
      "source": [
        "# tensorflow==1.15.0 to have same versions in all environments - dataflow, automl, ai-platform\n",
        "!pip install tensorflow==1.15.0 --ignore-installed\n",
        "# CSV to hold (IMAGE_PATH, LABEL) list.\n",
        "input_data_csv = automl_bucket_name + \"/input.csv\"\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "from tensorflow.python.lib.io import file_io\n",
        "import scripts.tcia_utils as tcia_utils\n",
        "\n",
        "# Get map of study_uid -\u003e file paths.\n",
        "path_list = file_io.get_matching_files(os.path.join(jpeg_folder, '*/*/*'))\n",
        "study_uid_to_file_paths = {}\n",
        "pattern = r'^{0}(?P\u003cstudy_uid\u003e[^/]+)/(?P\u003cseries_uid\u003e[^/]+)/(?P\u003cinstance_uid\u003e.*)'.format(jpeg_folder)\n",
        "for path in path_list:\n",
        "  match = re.search(pattern, path)\n",
        "  study_uid_to_file_paths[match.group('study_uid')] = path\n",
        "\n",
        "# Get map of study_uid -\u003e labels.\n",
        "study_uid_to_labels = tcia_utils.GetStudyUIDToLabelMap()\n",
        "\n",
        "# Join the two maps, output results to CSV in Google Cloud Storage.\n",
        "with file_io.FileIO(input_data_csv, 'w') as f:\n",
        "  writer = csv.writer(f, delimiter=',')\n",
        "  for study_uid, label in study_uid_to_labels.items():\n",
        "    if study_uid in study_uid_to_file_paths:\n",
        "      writer.writerow([study_uid_to_file_paths[study_uid], label])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JDxxEp3XnFp1"
      },
      "source": [
        "## Training\n",
        "\n",
        "***This section will focus on using AutoML through its API. AutoML can also be used through the user interface found [here](https://console.cloud.google.com/vision/). The below steps in this section can all be done through the web UI .***\n",
        "\n",
        "We will use [AutoML Vision ](https://cloud.google.com/automl/) to train the classification model. AutoML provides a fully managed solution for training the model. All we will do is input the list of input images and labels. The trained model in AutoML will be able to classify the mammography images as either \"2\" (scattered density) or \"3\" (heterogeneously dense).\n",
        "\n",
        "As a first step, we will create a AutoML dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "If7y8FkwZ1V-"
      },
      "outputs": [],
      "source": [
        "automl_dataset_display_name = \"MY_AUTOML_DATASET\" # @param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VPPZsQScaZm7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Path to AutoML API.\n",
        "AUTOML_API_URL = 'https://automl.googleapis.com/v1beta1'\n",
        "\n",
        "# Path to request creation of AutoML dataset.\n",
        "path = os.path.join(AUTOML_API_URL, 'projects', project_id, 'locations', location, 'datasets')\n",
        "\n",
        "# Headers (request in JSON format).\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "# Body (encoded in JSON format).\n",
        "config = {'display_name': automl_dataset_display_name, 'image_classification_dataset_metadata': {'classification_type': 'MULTICLASS'}}\n",
        "\n",
        "resp = authed_session.post(path, headers=headers, json=config)\n",
        "assert resp.status_code == 200, 'creating AutoML dataset, code: {0}, response: {1}'.format(resp.status_code, resp.text)\n",
        "print('Full response:\\n{0}'.format(resp.text))\n",
        "\n",
        "# Record the AutoML dataset name.\n",
        "response = json.loads(resp.text)\n",
        "automl_dataset_name = response['name']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I-pK9_jQewO9"
      },
      "source": [
        "Next, we will import the CSV that contains the list of (IMAGE_PATH, LABEL) list into AutoML. **Please ignore errors regarding an existing ground truth.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gD3GsUBee78l"
      },
      "outputs": [],
      "source": [
        "# Path to request import into AutoML dataset.\n",
        "path = os.path.join(AUTOML_API_URL, automl_dataset_name + ':importData')\n",
        "\n",
        "# Body (encoded in JSON format).\n",
        "config = {'input_config': {'gcs_source': {'input_uris': [input_data_csv]}}} \n",
        "\n",
        "resp = authed_session.post(path, headers=headers, json=config)\n",
        "assert resp.status_code == 200, 'error importing AutoML dataset, code: {0}, response: {1}'.format(resp.status_code, resp.text)\n",
        "print('Full response:\\n{0}'.format(resp.text))\n",
        "\n",
        "# Record operation_name so we can poll for it later.\n",
        "response = json.loads(resp.text)\n",
        "operation_name = response['name']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AtkvgdlhjFS2"
      },
      "source": [
        "The output of the previous step is an [operation](https://cloud.google.com/vision/automl/docs/models#get-operation) that will need to poll the status for. We will poll until the operation's \"done\" field is set to true. This will take a few minutes to complete so we will wait until completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MSXpdV20yYnZ"
      },
      "outputs": [],
      "source": [
        "path = os.path.join(AUTOML_API_URL, operation_name)\n",
        "timeout = time.time() + 40*60 # Wait up to 40 minutes.\n",
        "_ = wait_for_operation_completion(path, timeout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tUrg3P6EkqNS"
      },
      "source": [
        "Next, we will train the model to perform classification. We will set the training budget to be a maximum of 1hr (but this can be modified below). The cost of using AutoML can be found [here](https://cloud.google.com/vision/automl/pricing). Typically, the longer the model is trained for, the more accurate it will be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kUJnoVd8k_hy"
      },
      "outputs": [],
      "source": [
        "# Name of the model.\n",
        "model_display_name = \"MY_MODEL_NAME\" # @param\n",
        "\n",
        "# Training budget (1 hr).\n",
        "training_budget = 1 # @param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "voU4GkP6lWKE"
      },
      "outputs": [],
      "source": [
        "# Path to request import into AutoML dataset.\n",
        "path = os.path.join(AUTOML_API_URL, 'projects', project_id, 'locations', location, 'models')\n",
        "\n",
        "# Headers (request in JSON format).\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "# Body (encoded in JSON format).\n",
        "automl_dataset_id = automl_dataset_name.split('/')[-1]\n",
        "config = {'display_name': model_display_name, 'dataset_id': automl_dataset_id, 'image_classification_model_metadata': {'train_budget': training_budget}}\n",
        "\n",
        "resp = authed_session.post(path, headers=headers, json=config)\n",
        "assert resp.status_code == 200, 'error creating AutoML model, code: {0}, response: {1}'.format(resp.status_code, contenresp.text)\n",
        "print('Full response:\\n{0}'.format(resp.text))\n",
        "\n",
        "# Record operation_name so we can poll for it later.\n",
        "response = json.loads(resp.text)\n",
        "operation_name = response['name']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EADtTufcnPFf"
      },
      "source": [
        "The output of the previous step is also an [operation](https://cloud.google.com/vision/automl/docs/models#get-operation) that will need to poll the status of. We will poll until the operation's \"done\" field is set to true. This will take a few minutes to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vsl3iHa2niLo"
      },
      "outputs": [],
      "source": [
        "path = os.path.join(AUTOML_API_URL, operation_name)\n",
        "timeout = time.time() + 40*60 # Wait up to 40 minutes.\n",
        "sleep_time = 5*60 # Update each 5 minutes.\n",
        "response = wait_for_operation_completion(path, timeout, sleep_time)\n",
        "full_model_name = response['response']['name']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mI3oD2AeyYnq"
      },
      "outputs": [],
      "source": [
        "# google.cloud.automl to make api calls to Cloud AutoML\n",
        "!pip install google-cloud-automl\n",
        "from google.cloud import automl_v1\n",
        "client = automl_v1.AutoMlClient()\n",
        "response = client.deploy_model(full_model_name)\n",
        "print(u'Model deployment finished. {}'.format(response.result()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZdkETXQqqggh"
      },
      "source": [
        "Next, we will check out the accuracy metrics for the trained model. The following command will return the [AUC (ROC)](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc), [precision](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall) and [recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall) for the model, for various ML classification thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "efrr6X7zrvW2"
      },
      "outputs": [],
      "source": [
        "# Path to request to get model accuracy metrics.\n",
        "path = os.path.join(AUTOML_API_URL, full_model_name,  'modelEvaluations')\n",
        "\n",
        "resp = authed_session.get(path)\n",
        "assert resp.status_code == 200, 'error getting AutoML model evaluations, code: {0}, response: {1}'.format(resp.status_code, resp.text)\n",
        "print('Full response:\\n{0}'.format(resp.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BYbmGGoooIws"
      },
      "source": [
        "## Inference\n",
        "\n",
        "To allow medical imaging ML models to be easily integrated into clinical workflows, an *inference module* can be used. A standalone modality, a PACS system or a DICOM router can push DICOM instances into Cloud Healthcare [DICOM stores](https://cloud.google.com/healthcare/docs/introduction), allowing ML models to be triggered for inference. This inference results can then be structured into various DICOM formats (e.g. DICOM [structured reports](http://dicom.nema.org/MEDICAL/Dicom/2014b/output/chtml/part20/sect_A.3.html)) and stored in the Cloud Healthcare API, which can then be retrieved by the customer.\n",
        "\n",
        "The inference module is built as a [Docker](https://www.docker.com/) container and deployed using [Kubernetes](https://kubernetes.io/), allowing you to easily scale your deployment. The dataflow for inference can look as follows (see corresponding diagram below):\n",
        "\n",
        "1. Client application uses [STOW-RS](ftp://dicom.nema.org/medical/Dicom/2013/output/chtml/part18/sect_6.6.html) to push a new DICOM instance to the Cloud Healthcare DICOMWeb API.\n",
        "\n",
        "2. The insertion of the DICOM instance triggers a [Cloud Pubsub](https://cloud.google.com/pubsub/) message to be published. The *inference module* will pull incoming Pubsub messages and will recieve a message for the previously inserted DICOM instance. \n",
        "\n",
        "3. The *inference module* will retrieve the instance in JPEG format from the Cloud Healthcare API using [WADO-RS](ftp://dicom.nema.org/medical/Dicom/2013/output/chtml/part18/sect_6.5.html).\n",
        "\n",
        "4. The *inference module* will send the JPEG bytes to the model hosted on AutoML.\n",
        "\n",
        "5. AutoML will return the prediction back to the  *inference module*.\n",
        "\n",
        "6. The *inference module* will package the prediction into a DICOM instance. This can potentially be a DICOM structured report, [presentation state](ftp://dicom.nema.org/MEDICAL/dicom/2014b/output/chtml/part03/sect_A.33.html), or even burnt text on the image. In this codelab, we will focus on just DICOM structured reports. The structured report is then stored back in the Cloud Healthcare API using STOW-RS.\n",
        "\n",
        "7. The client application can query for (or retrieve) the structured report by using [QIDO-RS](http://dicom.nema.org/dicom/2013/output/chtml/part18/sect_6.7.html) or WADO-RS. Pubsub can also be used by the client application to poll for the newly created DICOM structured report instance.\n",
        "\n",
        "![Inference data flow](images/automl_inference_pipeline.png)\n",
        "\n",
        "\n",
        "To begin, we will create a new DICOM store that will store our inference source (DICOM mammography instance) and results (DICOM structured report). In order to enable Pubsub notifications to be triggered on inserted instances, we will give the DICOM store a Pubsub channel to publish on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "r7VrAx6B0TQK"
      },
      "outputs": [],
      "source": [
        "# Pubsub config.\n",
        "pubsub_topic_id = \"MY_PUBSUB_TOPIC_ID\" # @param\n",
        "pubsub_subscription_id = \"MY_PUBSUB_SUBSRIPTION_ID\" # @param\n",
        "\n",
        "# DICOM Store for store DICOM used for inference.\n",
        "inference_dicom_store_id = \"MY_INFERENCE_DICOM_STORE\" # @param\n",
        "\n",
        "pubsub_subscription_name = \"projects/\" + project_id + \"/subscriptions/\" + pubsub_subscription_id\n",
        "inference_dicom_store_path = dicom_path.FromPath(dicom_store_path, store_id=inference_dicom_store_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "v10GgraT7XbO"
      },
      "outputs": [],
      "source": [
        "%%bash -s {pubsub_topic_id} {pubsub_subscription_id} {project_id} {location} {dataset_id} {inference_dicom_store_id}\n",
        "\n",
        "# Create Pubsub channel.\n",
        "gcloud beta pubsub topics create $1\n",
        "gcloud beta pubsub subscriptions create $2 --topic $1\n",
        "\n",
        "# Create a Cloud Healthcare DICOM store that published on given Pubsub topic.\n",
        "TOKEN=`gcloud beta auth application-default print-access-token`\n",
        "NOTIFICATION_CONFIG=\"{notification_config: {pubsub_topic: \\\"projects/$3/topics/$1\\\"}}\"\n",
        "curl -s -X POST -H \"Content-Type: application/json\" -d \"${NOTIFICATION_CONFIG}\" https://healthcare.googleapis.com/v1/projects/$3/locations/$4/datasets/$5/dicomStores?access_token=${TOKEN}\\\u0026dicom_store_id=$6\n",
        "\n",
        "# Enable Cloud Healthcare API to publish on given Pubsub topic.\n",
        "PROJECT_NUMBER=`gcloud projects describe $3 | grep projectNumber | sed 's/[^0-9]//g'`\n",
        "SERVICE_ACCOUNT=\"service-${PROJECT_NUMBER}@gcp-sa-healthcare.iam.gserviceaccount.com\"\n",
        "gcloud beta pubsub topics add-iam-policy-binding $1 --member=\"serviceAccount:${SERVICE_ACCOUNT}\" --role=\"roles/pubsub.publisher\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VRs-EWOf_VIU"
      },
      "source": [
        "Next, we will building the *inference module* using [Cloud Build API](https://cloud.google.com/cloud-build/docs/api/reference/rest/). This will create a Docker container that will be stored in [Google Container Registry](https://cloud.google.com/container-registry/). The inference module code is found in *[inference.py](./scripts/inference/inference.py)*. The build script used to build the Docker container for this module is *[cloudbuild.yaml](./scripts/inference/cloudbuild.yaml)*. Progress of build may be found on [cloud build dashboard](https://console.cloud.google.com/cloud-build/builds?project=)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nolumVGiL47X"
      },
      "outputs": [],
      "source": [
        "%%bash -s {project_id}\n",
        "PROJECT_ID=$1\n",
        "\n",
        "gcloud builds submit --config scripts/inference/cloudbuild.yaml --timeout 1h scripts/inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YtufvIOrdnP7"
      },
      "source": [
        "Next, we will deploy the *inference module* to Kubernetes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oIWLkRKleFJS"
      },
      "source": [
        "Then we create a Kubernetes Cluster and a Deployment for the *inference module*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uJHRFPvjeDnZ"
      },
      "outputs": [],
      "source": [
        "%%bash -s {project_id} {location} {pubsub_subscription_name} {full_model_name} {inference_dicom_store_path}\n",
        "gcloud container clusters create inference-module --region=$2 --scopes https://www.googleapis.com/auth/cloud-platform --num-nodes=1\n",
        "\n",
        "PROJECT_ID=$1\n",
        "SUBSCRIPTION_PATH=$3\n",
        "MODEL_PATH=$4\n",
        "INFERENCE_DICOM_STORE_PATH=$5\n",
        "\n",
        "cat \u003c\u003cEOF | kubectl create -f -\n",
        "apiVersion: extensions/v1beta1\n",
        "kind: Deployment\n",
        "metadata:\n",
        "  name: inference-module\n",
        "  namespace: default\n",
        "spec:\n",
        "  replicas: 1\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: inference-module\n",
        "    spec:\n",
        "      containers:\n",
        "        - name: inference-module\n",
        "          image: gcr.io/${PROJECT_ID}/inference-module:latest\n",
        "          command:\n",
        "            - \"/opt/inference_module/bin/inference_module\"\n",
        "            - \"--subscription_path=${SUBSCRIPTION_PATH}\"\n",
        "            - \"--model_path=${MODEL_PATH}\"\n",
        "            - \"--dicom_store_path=${INFERENCE_DICOM_STORE_PATH}\"\n",
        "            - \"--prediction_service=AutoML\"\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dgUu3dJN8spl"
      },
      "source": [
        "Next, we will store a mammography DICOM instance from the TCIA dataset to the DICOM store. This is the image that we will request inference for. Pushing this instance to the DICOM store will result in a Pubsub message, which will trigger the *inference module*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9CG0225T8rw2"
      },
      "outputs": [],
      "source": [
        "# DICOM Study/Series UID of input mammography image that we'll push for inference.\n",
        "input_mammo_study_uid = \"1.3.6.1.4.1.9590.100.1.2.85935434310203356712688695661986996009\"\n",
        "input_mammo_series_uid = \"1.3.6.1.4.1.9590.100.1.2.374115997511889073021386151921807063992\"\n",
        "input_mammo_instance_uid = \"1.3.6.1.4.1.9590.100.1.2.289923739312470966435676008311959891294\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SRtBZf5N-ou8"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "client = storage.Client()\n",
        "bucket = client.bucket('gcs-public-data--healthcare-tcia-cbis-ddsm', user_project=project_id)\n",
        "blob = bucket.blob(\"dicom/{}/{}/{}.dcm\".format(input_mammo_study_uid,input_mammo_series_uid,input_mammo_instance_uid))\n",
        "blob.download_to_filename(\"example.dcm\")\n",
        "with open(\"example.dcm\", 'rb') as dcm:\n",
        "  dcm_content = dcm.read()\n",
        "study_path = dicom_path.FromPath(inference_dicom_store_path, study_uid=input_mammo_study_uid)\n",
        "url = os.path.join(HEALTHCARE_API_URL, str(study_path))\n",
        "headers = {'Content-Type': 'application/dicom'}\n",
        "authed_session.post(url, headers=headers, data=dcm_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SrPN8rW1wxcg"
      },
      "source": [
        "You should be able to observe the *inference module*'s logs by running the following command. In the logs, you should observe that the inference module successfully recieved the the Pubsub message and ran inference on the DICOM instance. The logs should also include the inference results. It can take a few minutes for the Kubernetes deployment to start up, so you many need to run this a few times. The logs should also include the inference results. It can take a few minutes for the Kubernetes deployment to start up, so you many need to run this a few times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "muiDpFTuxMOk"
      },
      "outputs": [],
      "source": [
        "!kubectl logs -l app=inference-module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l9ibc2yayM_j"
      },
      "source": [
        "You can also query the Cloud Healthcare DICOMWeb API (using QIDO-RS) to see that the DICOM structured report has been inserted for the study. The structured report contents can be found under tag **\"0040A730\"**. \n",
        "\n",
        "You can optionally also use WADO-RS to recieve the instance (e.g. for viewing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "grWtbIQCyL0r"
      },
      "outputs": [],
      "source": [
        "%%bash -s {study_path}\n",
        "\n",
        "TOKEN=`gcloud beta auth application-default print-access-token`\n",
        "\n",
        "# QIDO-RS should return two results in JSON response. One for the original DICOM\n",
        "# instance, and one for the Strucured Report containing the inference results.\n",
        "curl -s https://healthcare.googleapis.com/v1/$1/instances?includefield=all\\\u0026access_token=${TOKEN} | python -m json.tool"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "breast_density_auto_ml.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
