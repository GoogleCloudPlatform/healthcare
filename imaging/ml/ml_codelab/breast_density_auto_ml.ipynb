{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VbkyG_K1LSnC"
      },
      "source": [
        "Copyright 2018 Google Inc.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q8ZHSBelKd8V"
      },
      "source": [
        "# Training/Inference on Breast Density Classification Model on AutoML Vision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XCClehsPK2Jq"
      },
      "source": [
        "The goal of this tutorial is to train, deploy and run inference on a breast density classification model. Breast density is thought to be a factor for an increase in the risk for breast cancer. This will emphasize using the [Cloud Healthcare API](https://cloud.google.com/healthcare/) in order to store, retreive and transcode medical images (in DICOM format) in a managed and scalable way. This tutorial will focus on using [Cloud AutoML Vision](https://cloud.google.com/vision/automl/docs/beginners-guide) to scalably train and serve the model. \n",
        "\n",
        "**Note: This is the AutoML version of the Cloud ML Engine Codelab found [here](./breast_density_cloud_ml.ipynb).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u1OUa9pCPtqu"
      },
      "source": [
        "## Requirements\n",
        "- A Google Cloud project.\n",
        "- Project has [Cloud Healthcare API](https://cloud.google.com/healthcare/docs/quickstart) enabled (**Note: You will need to be [whitelisted](https://cloud.google.com/healthcare/) for this product as it is in Alpha**).\n",
        "- Project has [Cloud AutoML API ](https://cloud.google.com/vision/automl/docs/quickstart)enabled.\n",
        "- Project has [Cloud Build API](https://cloud.google.com/cloud-build/docs/quickstart-docker) enabled.\n",
        "- Environment has [Google Application Default Credentials](https://cloud.google.com/docs/authentication/production#providing_service_account_credentials) set up. This is already set up if running on [Cloud Datalab](https://cloud.google.com/datalab/).\n",
        "- Environment has [gcloud](https://cloud.google.com/sdk/gcloud/) and [gsutil](https://cloud.google.com/storage/docs/gsutil) installed. This is already set up if running on [Cloud Datalab](https://cloud.google.com/datalab/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qL-vcMmZRITm"
      },
      "source": [
        "## Input Dataset\n",
        "\n",
        "The dataset that will be used for training is the [TCIA CBIS-DDSM](https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM) dataset. This dataset contains ~2500 mammography images in DICOM format. Each image is given a [BI-RADS breast density ](https://breast-cancer.ca/densitbi-rads/) score from 1 to 4. In this tutorial, we will build a binary classifier that distinguishes between breast density \"2\" (*scattered density*) and \"3\" (*heterogeneously dense*). These are the two most common and variably assigned scores. In the literature, this is said to be [particularly difficult for radiologists to consistently distinguish](https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.12683)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4WiSDYeS3faP"
      },
      "source": [
        "### Store TCIA Dataset in Cloud Healthcare API\n",
        "\n",
        "The [TCIA REST API ](https://wiki.cancerimagingarchive.net/display/Public/TCIA+Programmatic+Interface+%28REST+API%29+Usage+Guide)  will be used to fetch the images. The TCIA requires an API key which can be retreived by following the instructions in the **Getting Started with the TCIA API** section of the previous link. Once you receive the API key, assign it below (**NOTE: TCIA does not support self-registration, so expect some turn-around time until you get the key**). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LjzzbUQOc5l0"
      },
      "outputs": [],
      "source": [
        "tcia_api_key = \"MY_KEY\" #@param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mhL5x08vYdy9"
      },
      "source": [
        "We need to create a Cloud Healthcare API Dataset and Dicom Store to store the the DICOM instances sourced from TCIA. Please refer [here](https://cloud.google.com/healthcare/docs/introduction) for a description of the Cloud Healthcare API data hierarchy. Add your parameters for Cloud Healthcare API below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NSr5StGBZkYd"
      },
      "outputs": [],
      "source": [
        "project_id = \"MY_PROJECT\" # @param\n",
        "location = \"us-central1\" # @param\n",
        "dataset_id = \"MY_DATASET\" # @param\n",
        "dicom_store_id = \"MY_DICOM_STORE\" # @param\n",
        "\n",
        "\n",
        "# Input data used by AutoML must be in a bucket with the following format.\n",
        "automl_bucket_name = \"gs://\" + project_id + \"-vcm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vgBA16ptbacM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import httplib2\n",
        "import os\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Path to Cloud Healthcare API.\n",
        "HEALTHCARE_API_URL = 'https://healthcare.googleapis.com/v1beta1'\n",
        "\n",
        "http = httplib2.Http()\n",
        "http = GoogleCredentials.get_application_default().authorize(http)\n",
        "\n",
        "# Create Cloud Healthcare API dataset.\n",
        "path = os.path.join(HEALTHCARE_API_URL, 'projects', project_id, 'locations', location, 'datasets?dataset_id=' + dataset_id)\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "resp, content = http.request(path, method='POST', headers=headers)\n",
        "assert resp.status == 200, 'error creating Dataset, code: {0}, response: {1}'.format(resp.status, content)\n",
        "print('Full response:\\n{0}'.format(content))\n",
        "\n",
        "# Create Cloud Healthcare API DICOM store.\n",
        "path = os.path.join(HEALTHCARE_API_URL, 'projects', project_id, 'locations', location, 'datasets', dataset_id, 'dicomStores?dicom_store_id=' + dicom_store_id)\n",
        "resp, content = http.request(path, method='POST', headers=headers)\n",
        "assert resp.status == 200, 'error creating DICOM store, code: {0}, response: {1}'.format(resp.status, content)\n",
        "print('Full response:\\n{0}'.format(content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HKsCfbXorosM"
      },
      "source": [
        "Next, we are going to transfer the DICOM instances from the TCIA API to the Cloud Healthcare API. We have added a helper script called *[store_tcia_in_hc_api.py](./scripts/store_tcia_in_hc_api.py)* to do this. Internally, this uses the STOW-RS DICOMWeb protocol (implemented as DicomWebPost in Cloud Healthcare API).\n",
        "\n",
        "Note: We are transfering \u003e100GB of data so this will take some time to complete (this takes ~30min on n1-standard-4 machine-type). There is an optional *--max_concurrency* flag that allows you to modify the rate of data transfer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "shhPzNFArpHH"
      },
      "outputs": [],
      "source": [
        "# Store DICOM instances in Cloud Healthcare API.\n",
        "!python -m scripts.store_tcia_in_hc_api --project_id=$project_id --location=$location --dataset_id=$dataset_id --dicom_store_id=$dicom_store_id --tcia_api_key=$tcia_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mjFm5w356Z6t"
      },
      "source": [
        "### Explore the Cloud Healthcare DICOM dataset (optional)\n",
        "\n",
        "This is an optional section to explore the Cloud Healthcare DICOM dataset. In the following code, we simply just list the studies that we have loaded into the Cloud Healthcare API. You can modify the *num_of_studies_to_print* parameter to print as many studies as desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RUjYgoym7MZN"
      },
      "outputs": [],
      "source": [
        "num_of_studies_to_print = 2 # @param\n",
        "\n",
        "\n",
        "path = os.path.join(HEALTHCARE_API_URL, 'projects', project_id, 'locations', location, 'datasets', dataset_id, 'dicomStores', dicom_store_id, 'dicomWeb', 'studies')\n",
        "resp, content = http.request(path, method='GET')\n",
        "assert resp.status == 200, 'error querying Dataset, code: {0}, response: {1}'.format(resp.status, content)\n",
        "response = json.loads(content)\n",
        "\n",
        "print(json.dumps(response[:num_of_studies_to_print], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qpkeGeAsb-ec"
      },
      "source": [
        "## Convert DICOM to JPEG\n",
        "\n",
        "The ML model that we will build requires that the dataset be in JPEG. We will leverage the Cloud Healthcare API to transcode DICOM to JPEG.\n",
        "\n",
        "First we will create a [Google Cloud Storage](https://cloud.google.com/storage/) bucket to hold the output JPEG files. Next, we will use the ExportDicomData API to transform the DICOMs to JPEGs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sXB41cvKeLZj"
      },
      "outputs": [],
      "source": [
        "# Folder to store input images for AutoML Vision.\n",
        "jpeg_folder = automl_bucket_name + \"/images/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UW3Y6Pd6d1Ey"
      },
      "outputs": [],
      "source": [
        "%%bash -s {project_id} {location} {automl_bucket_name}\n",
        "# Create bucket.\n",
        "gsutil -q mb -c regional -l $2 $3\n",
        "\n",
        "# Allow Cloud Healthcare API to write to bucket.\n",
        "PROJECT_NUMBER=`gcloud projects describe $1 | grep projectNumber | sed 's/[^0-9]//g'`\n",
        "SERVICE_ACCOUNT=\"service-${PROJECT_NUMBER}@gcp-sa-healthcare.iam.gserviceaccount.com\"\n",
        "gsutil -q iam ch serviceAccount:${SERVICE_ACCOUNT}:objectCreator $3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aIfKnxqHjE9A"
      },
      "source": [
        "Next we will convert the DICOMs to JPEGs using the ExportDicomData API. This is an asynchronous call that returns an Operation name. You can use the Operation name to poll the status of the operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2Ic_OPeVjM5i"
      },
      "outputs": [],
      "source": [
        "# Path to export DICOM data.\n",
        "dicom_store_url = os.path.join(HEALTHCARE_API_URL, 'projects', project_id, 'locations', location, 'datasets', dataset_id, 'dicomStores', dicom_store_id)\n",
        "path = dicom_store_url + \":export\"\n",
        "\n",
        "# Headers (send request in JSON format).\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "# Body (encoded in JSON format).\n",
        "output_config = {'output_config': {'gcs_destination': {'uri_prefix': jpeg_folder, 'mime_type': 'image/jpeg; transfer-syntax=1.2.840.10008.1.2.4.50'}}}\n",
        "body = json.dumps(output_config)\n",
        "\n",
        "resp, content = http.request(path, method='POST', headers=headers, body=body)\n",
        "assert resp.status == 200, 'error exporting to JPEG, code: {0}, response: {1}'.format(resp.status, content)\n",
        "print('Full response:\\n{0}'.format(content))\n",
        "\n",
        "# Record operation_name so we can poll for it later.\n",
        "response = json.loads(content)\n",
        "operation_name = response['name']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pVhp8IdVkVH3"
      },
      "source": [
        "We will use the Operation ID returned from the previous command to poll the status of ExportDicomData. This will take a few minutes to complete so we will wait until completion. When the operation is complete, the operation's *done* field will be set to true.\n",
        "\n",
        "Meanwhile, you should be able to observe the JPEG images being added to your Google Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "G4GE7pzsl-lP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def wait_for_operation_completion(path, timeout): \n",
        "  success = False\n",
        "  while time.time() \u003c timeout:\n",
        "    print('Waiting for operation completion...')\n",
        "    resp, content = http.request(path, method='GET')\n",
        "    assert resp.status == 200, 'error polling for Operation results, code: {0}, response: {1}'.format(resp.status, content)\n",
        "    response = json.loads(content)\n",
        "    if 'done' in response:\n",
        "      if response['done'] == True and 'error' not in response:\n",
        "        success = True;\n",
        "      break\n",
        "    time.sleep(30)\n",
        "\n",
        "  print('Full response:\\n{0}'.format(content))      \n",
        "  assert success, \"operation did not complete successfully in time limit\"\n",
        "  print('Success!')\n",
        "  return response\n",
        "  \n",
        "timeout = time.time() + 10*60 # Wait up to 10 minutes.\n",
        "path = os.path.join(HEALTHCARE_API_URL, operation_name)\n",
        "_ = wait_for_operation_completion(path, timeout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "liblamUVyWZw"
      },
      "source": [
        "Next, we will join the training data stored in Google Cloud Storage with the labels in the TCIA website. The output of this step is a [CSV file that is input to AutoML](https://cloud.google.com/vision/automl/docs/prepare). This CSV contains a list of pairs of (IMAGE_PATH, LABEL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DgDI4Esw8bXT"
      },
      "outputs": [],
      "source": [
        "# CSV to hold (IMAGE_PATH, LABEL) list.\n",
        "input_data_csv = automl_bucket_name + \"/input.csv\"\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "from tensorflow.python.lib.io import file_io\n",
        "import scripts.tcia_utils as tcia_utils\n",
        "\n",
        "# Get map of study_uid -\u003e file paths.\n",
        "path_list = file_io.get_matching_files(os.path.join(jpeg_folder, '*/*/*'))\n",
        "study_uid_to_file_paths = {}\n",
        "pattern = r'^{0}(?P\u003cstudy_uid\u003e[^/]+)/(?P\u003cseries_uid\u003e[^/]+)/(?P\u003cinstance_uid\u003e.*)'.format(jpeg_folder)\n",
        "for path in path_list:\n",
        "  match = re.search(pattern, path)\n",
        "  study_uid_to_file_paths[match.group('study_uid')] = path\n",
        "\n",
        "# Get map of study_uid -\u003e labels.\n",
        "study_uid_to_labels = tcia_utils.GetStudyUIDToLabelMap()\n",
        "\n",
        "# Join the two maps, output results to CSV in Google Cloud Storage.\n",
        "with file_io.FileIO(input_data_csv, 'w') as f:\n",
        "  writer = csv.writer(f, delimiter=',')\n",
        "  for study_uid, label in study_uid_to_labels.iteritems():\n",
        "    if study_uid in study_uid_to_file_paths:\n",
        "      writer.writerow([study_uid_to_file_paths[study_uid], label])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JDxxEp3XnFp1"
      },
      "source": [
        "## Training\n",
        "\n",
        "***This section will focus on using AutoML through its API. AutoML can also be used through the user interface found [here](https://beta-dot-custom-vision.appspot.com/vision/). The below steps in this section can all be done through the web UI .***\n",
        "\n",
        "We will use [AutoML Vision ](https://cloud.google.com/automl/) to train the classification model. AutoML provides a fully managed solution for training the model. All we will do is input the list of input images and labels. The trained model in AutoML will be able to classify the mammography images as either \"2\" (scattered density) or \"3\" (heterogeneously dense).\n",
        "\n",
        "As a first step, we will create a AutoML dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "If7y8FkwZ1V-"
      },
      "outputs": [],
      "source": [
        "automl_dataset_display_name = \"MY_AUTOML_DATASET\" # @param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VPPZsQScaZm7"
      },
      "outputs": [],
      "source": [
        "# Path to AutoML API.\n",
        "AUTOML_API_URL = 'https://automl.googleapis.com/v1beta1'\n",
        "\n",
        "# Path to request creation of AutoML dataset.\n",
        "path = os.path.join(AUTOML_API_URL, 'projects', project_id, 'locations', location, 'datasets')\n",
        "\n",
        "# Headers (request in JSON format).\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "# Body (encoded in JSON format).\n",
        "config = {'display_name': automl_dataset_display_name, 'image_classification_dataset_metadata': {'classification_type': 'MULTICLASS'}}\n",
        "body = json.dumps(config)\n",
        "\n",
        "resp, content = http.request(path, method='POST', headers=headers, body=body)\n",
        "assert resp.status == 200, 'creating AutoML dataset, code: {0}, response: {1}'.format(resp.status, content)\n",
        "print('Full response:\\n{0}'.format(content))\n",
        "\n",
        "# Record the AutoML dataset name.\n",
        "response = json.loads(content)\n",
        "automl_dataset_name = response['name']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I-pK9_jQewO9"
      },
      "source": [
        "Next, we will import the CSV that contains the list of (IMAGE_PATH, LABEL) list into AutoML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gD3GsUBee78l"
      },
      "outputs": [],
      "source": [
        "# Path to request import into AutoML dataset.\n",
        "path = os.path.join(AUTOML_API_URL, automl_dataset_name + ':importData')\n",
        "\n",
        "# Body (encoded in JSON format).\n",
        "config = {'input_config': {'gcs_source': {'input_uris': [input_data_csv]}}} \n",
        "body = json.dumps(config)\n",
        "\n",
        "resp, content = http.request(path, method='POST', headers=headers, body=body)\n",
        "assert resp.status == 200, 'error importing AutoML dataset, code: {0}, response: {1}'.format(resp.status, content)\n",
        "print('Full response:\\n{0}'.format(content))\n",
        "\n",
        "# Record operation_name so we can poll for it later.\n",
        "response = json.loads(content)\n",
        "operation_name = response['name']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AtkvgdlhjFS2"
      },
      "source": [
        "The output of the previous step is an [operation](https://cloud.google.com/vision/automl/docs/models#get-operation) that will need to poll the status for. We will poll until the operation's \"done\" field is set to true. This will take a few minutes to complete so we will wait until completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "biHU0WoLjgYs"
      },
      "outputs": [],
      "source": [
        "path = os.path.join(AUTOML_API_URL, operation_name)\n",
        "timeout = time.time() + 10*60 # Wait up to 10 minutes.\n",
        "_ = wait_for_operation_completion(path, timeout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tUrg3P6EkqNS"
      },
      "source": [
        "Next, we will train the model to perform classification. We will set the training budget to be a maximum of 1hr (but this can be modified below). The cost of using AutoML can be found [here](https://cloud.google.com/vision/automl/pricing). Typically, the longer the model is trained for, the more accurate it will be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kUJnoVd8k_hy"
      },
      "outputs": [],
      "source": [
        "# Name of the model.\n",
        "model_display_name = \"MY_MODEL_NAME\" # @param\n",
        "\n",
        "# Training budget (1 hr).\n",
        "training_budget = 1 # @param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "voU4GkP6lWKE"
      },
      "outputs": [],
      "source": [
        "# Path to request import into AutoML dataset.\n",
        "path = os.path.join(AUTOML_API_URL, 'projects', project_id, 'locations', location, 'models')\n",
        "\n",
        "# Headers (request in JSON format).\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "# Body (encoded in JSON format).\n",
        "automl_dataset_id = automl_dataset_name.split('/')[-1]\n",
        "config = {'display_name': model_display_name, 'dataset_id': automl_dataset_id, 'image_classification_model_metadata': {'train_budget': training_budget}}\n",
        "body = json.dumps(config)\n",
        "\n",
        "resp, content = http.request(path, method='POST', headers=headers, body=body)\n",
        "assert resp.status == 200, 'error creating AutoML model, code: {0}, response: {1}'.format(resp.status, content)\n",
        "print('Full response:\\n{0}'.format(content))\n",
        "\n",
        "# Record operation_name so we can poll for it later.\n",
        "response = json.loads(content)\n",
        "operation_name = response['name']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EADtTufcnPFf"
      },
      "source": [
        "The output of the previous step is also an [operation](https://cloud.google.com/vision/automl/docs/models#get-operation) that will need to poll the status of. We will poll until the operation's \"done\" field is set to true. This will take a few minutes to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vsl3iHa2niLo"
      },
      "outputs": [],
      "source": [
        "path = os.path.join(AUTOML_API_URL, operation_name)\n",
        "timeout = time.time() + 20*60 # Wait up to 20 minutes.\n",
        "response = wait_for_operation_completion(path, timeout)\n",
        "full_model_name = response['response']['name']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZdkETXQqqggh"
      },
      "source": [
        "Next, we will check out the accuracy metrics for the trained model. The following command will return the [AUC (ROC)](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc), [precision](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall) and [recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall) for the model, for various ML classification thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "efrr6X7zrvW2"
      },
      "outputs": [],
      "source": [
        "# Path to request to get model accuracy metrics.\n",
        "path = os.path.join(AUTOML_API_URL, full_model_name,  'modelEvaluations')\n",
        "\n",
        "resp, content = http.request(path)\n",
        "assert resp.status == 200, 'error getting AutoML model evaluations, code: {0}, response: {1}'.format(resp.status, content)\n",
        "print('Full response:\\n{0}'.format(content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y63nMUukszpM"
      },
      "source": [
        "**Reminder:** AutoML can also be used through the user interface found [here](https://beta-dot-custom-vision.appspot.com/vision/). You can use this to view how well the trained model performs (e.g. confusion matrices).\n",
        "\n",
        "In the next section, we will deploy the model for predictions/inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BYbmGGoooIws"
      },
      "source": [
        "## Inference\n",
        "\n",
        "To allow medical imaging ML models to be easily integrated into clinical workflows, an *inference module* can be used. A standalone modality, a PACS system or a DICOM router can push DICOM instances into Cloud Healthcare [DICOM stores](https://cloud.google.com/healthcare/docs/introduction), allowing ML models to be triggered for inference. This inference results can then be structured into various DICOM formats (e.g. DICOM [structured reports](http://dicom.nema.org/MEDICAL/Dicom/2014b/output/chtml/part20/sect_A.3.html)) and stored in the Cloud Healthcare API, which can then be retrieved by the customer.\n",
        "\n",
        "The inference module is built as a [Docker](https://www.docker.com/) container and deployed using [Kubernetes](https://kubernetes.io/), allowing you to easily scale your deployment. The dataflow for inference can look as follows (see corresponding diagram below):\n",
        "\n",
        "1. Client application uses [STOW-RS](ftp://dicom.nema.org/medical/Dicom/2013/output/chtml/part18/sect_6.6.html) to push a new DICOM instance to the Cloud Healthcare DICOMWeb API.\n",
        "\n",
        "2. The insertion of the DICOM instance triggers a [Cloud Pubsub](https://cloud.google.com/pubsub/) message to be published. The *inference module* will pull incoming Pubsub messages and will recieve a message for the previously inserted DICOM instance. \n",
        "\n",
        "3. The *inference module* will retrieve the instance in JPEG format from the Cloud Healthcare API using [WADO-RS](ftp://dicom.nema.org/medical/Dicom/2013/output/chtml/part18/sect_6.5.html).\n",
        "\n",
        "4. The *inference module* will send the JPEG bytes to the model hosted on AutoML.\n",
        "\n",
        "5. AutoML will return the prediction back to the  *inference module*.\n",
        "\n",
        "6. The *inference module* will package the prediction into a DICOM instance. This can potentially be a DICOM structured report, [presentation state](ftp://dicom.nema.org/MEDICAL/dicom/2014b/output/chtml/part03/sect_A.33.html), or even burnt text on the image. In this codelab, we will focus on just DICOM structured reports. The structured report is then stored back in the Cloud Healthcare API using STOW-RS.\n",
        "\n",
        "7. The client application can query for (or retrieve) the structured report by using [QIDO-RS](http://dicom.nema.org/dicom/2013/output/chtml/part18/sect_6.7.html) or WADO-RS. Pubsub can also be used by the client application to poll for the newly created DICOM structured report instance.\n",
        "\n",
        "![Inference data flow](images/automl_inference_pipeline.png)\n",
        "\n",
        "\n",
        "To begin, we will create a new DICOM store that will store our inference source (DICOM mammography instance) and results (DICOM structured report). In order to enable Pubsub notifications to be triggered on inserted instances, we will give the DICOM store a Pubsub channel to publish on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "r7VrAx6B0TQK"
      },
      "outputs": [],
      "source": [
        "# Pubsub config.\n",
        "pubsub_topic_id = \"MY_PUBSUB_TOPIC_ID\" # @param\n",
        "pubsub_subscription_id = \"MY_PUBSUB_SUBSRIPTION_ID\" # @param\n",
        "\n",
        "# DICOM Store for store DICOM used for inference.\n",
        "inference_dicom_store_id = \"MY_INFERENCE_DICOM_STORE\" # @param\n",
        "\n",
        "pubsub_subscription_name = \"projects/\" + project_id + \"/subscriptions/\" + pubsub_subscription_id\n",
        "inference_dicom_store_name = \"projects/\" + project_id + \"/locations/\" + location + \"/datasets/\" + dataset_id + \"/dicomStores/\" + inference_dicom_store_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "v10GgraT7XbO"
      },
      "outputs": [],
      "source": [
        "%%bash -s {pubsub_topic_id} {pubsub_subscription_id} {project_id} {location} {dataset_id} {inference_dicom_store_id}\n",
        "\n",
        "# Create Pubsub channel.\n",
        "gcloud beta pubsub topics create $1\n",
        "gcloud beta pubsub subscriptions create $2 --topic $1\n",
        "\n",
        "# Create a Cloud Healthcare DICOM store that published on given Pubsub topic.\n",
        "TOKEN=`gcloud beta auth application-default print-access-token`\n",
        "NOTIFICATION_CONFIG=\"{notification_config: {pubsub_topic: \\\"projects/$3/topics/$1\\\"}}\"\n",
        "curl -s -X POST -H \"Content-Type: application/json\" -d \"${NOTIFICATION_CONFIG}\" https://healthcare.googleapis.com/v1beta1/projects/$3/locations/$4/datasets/$5/dicomStores?access_token=${TOKEN}\\\u0026dicom_store_id=$6\n",
        "\n",
        "# Enable Cloud Healthcare API to publish on given Pubsub topic.\n",
        "PROJECT_NUMBER=`gcloud projects describe $3 | grep projectNumber | sed 's/[^0-9]//g'`\n",
        "SERVICE_ACCOUNT=\"service-${PROJECT_NUMBER}@gcp-sa-healthcare.iam.gserviceaccount.com\"\n",
        "gcloud beta pubsub topics add-iam-policy-binding $1 --member=\"serviceAccount:${SERVICE_ACCOUNT}\" --role=\"roles/pubsub.publisher\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VRs-EWOf_VIU"
      },
      "source": [
        "Next, we will building the *inference module* using [Cloud Build API](https://cloud.google.com/cloud-build/docs/api/reference/rest/). This will create a Docker container that will be stored in [Google Container Registry](https://cloud.google.com/container-registry/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nolumVGiL47X"
      },
      "outputs": [],
      "source": [
        "%%bash -s {project_id}\n",
        "PROJECT_ID=$1\n",
        "\n",
        "gcloud builds submit --config scripts/inference/cloudbuild.yaml --timeout 1h scripts/inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YtufvIOrdnP7"
      },
      "source": [
        "Next, we will deploy the *inference module* to Kubernetes. To do this, we first need to install [kubectl](https://https://kubernetes.io/docs/reference/kubectl/overview/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FFz-ESYPL44v"
      },
      "outputs": [],
      "source": [
        "!gcloud -q components install kubectl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oIWLkRKleFJS"
      },
      "source": [
        "Then we create a Kubernetes Cluster and a Deployment for the *inference module*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uJHRFPvjeDnZ"
      },
      "outputs": [],
      "source": [
        "%%bash -s {project_id} {location} {pubsub_subscription_name} {full_model_name} {inference_dicom_store_name}\n",
        "gcloud container clusters create inference-module --region=$2 --scopes https://www.googleapis.com/auth/cloud-platform --num-nodes=1\n",
        "\n",
        "PROJECT_ID=$1\n",
        "SUBSCRIPTION_PATH=$3\n",
        "MODEL_PATH=$4\n",
        "INFERENCE_DICOM_STORE_NAME=$5\n",
        "\n",
        "cat \u003c\u003cEOF | kubectl create -f -\n",
        "apiVersion: extensions/v1beta1\n",
        "kind: Deployment\n",
        "metadata:\n",
        "  name: inference-module\n",
        "  namespace: default\n",
        "spec:\n",
        "  replicas: 1\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: inference-module\n",
        "    spec:\n",
        "      containers:\n",
        "        - name: inference-module\n",
        "          image: gcr.io/${PROJECT_ID}/inference-module:latest\n",
        "          command:\n",
        "            - \"/opt/inference_module/bin/inference_module\"\n",
        "            - \"--subscription_path=${SUBSCRIPTION_PATH}\"\n",
        "            - \"--model_path=${MODEL_PATH}\"\n",
        "            - \"--dicom_store_path=${INFERENCE_DICOM_STORE_NAME}\"\n",
        "            - \"--prediction_service=AutoML\"\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dgUu3dJN8spl"
      },
      "source": [
        "Next, we will store a mammography DICOM instance from the TCIA dataset to the DICOM store. This is the image that we will request inference for. Pushing this instance to the DICOM store will result in a Pubsub message, which will trigger the *inference module*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9CG0225T8rw2"
      },
      "outputs": [],
      "source": [
        "# DICOM Study/Series UID of input mammography image that we'll push for inference.\n",
        "input_mammo_study_uid = \"1.3.6.1.4.1.9590.100.1.2.85935434310203356712688695661986996009\"\n",
        "input_mammo_series_uid = \"1.3.6.1.4.1.9590.100.1.2.374115997511889073021386151921807063992\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SRtBZf5N-ou8"
      },
      "outputs": [],
      "source": [
        "%%bash -s {project_id} {location} {dataset_id} {inference_dicom_store_id} {tcia_api_key} {input_mammo_study_uid}\n",
        "\n",
        "# Store input mammo image into Cloud Healthcare DICOMWeb API.\n",
        "python -m scripts.store_tcia_in_hc_api --project_id=$1 --location=$2 --dataset_id=$3 --dicom_store_id=$4 --tcia_api_key=$5 --has_study_uid=$6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SrPN8rW1wxcg"
      },
      "source": [
        "You should be able to observe the *inference module*'s logs by running the following command. In the logs, you should observe that the inference module successfully recieved the the Pubsub message and ran inference on the DICOM instance. The logs should also include the inference results. It can take a few minutes for the Kubernetes deployment to start up, so you many need to run this a few times. The logs should also include the inference results. It can take a few minutes for the Kubernetes deployment to start up, so you many need to run this a few times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "muiDpFTuxMOk"
      },
      "outputs": [],
      "source": [
        "!kubectl logs -l app=inference-module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l9ibc2yayM_j"
      },
      "source": [
        "You can also query the Cloud Healthcare DICOMWeb API (using QIDO-RS) to see that the DICOM structured report has been inserted for the study. The structured report contents can be found under tag **\"0040A730\"**. \n",
        "\n",
        "You can optionally also use WADO-RS to recieve the instance (e.g. for viewing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "grWtbIQCyL0r"
      },
      "outputs": [],
      "source": [
        "%%bash -s {project_id} {location} {dataset_id} {inference_dicom_store_id} {input_mammo_study_uid}\n",
        "\n",
        "TOKEN=`gcloud beta auth application-default print-access-token`\n",
        "\n",
        "# QIDO-RS should return two results in JSON response. One for the original DICOM\n",
        "# instance, and one for the Strucured Report containing the inference results.\n",
        "curl -s https://healthcare.googleapis.com/v1beta1/projects/$1/locations/$2/datasets/$3/dicomStores/$4/dicomWeb/studies/$5/instances?includefield=all\\\u0026access_token=${TOKEN} | python -m json.tool"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "breast_density_auto_ml.ipynb",
      "provenance": [],
      "version": "0.3.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
