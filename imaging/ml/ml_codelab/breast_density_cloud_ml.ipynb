{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbkyG_K1LSnC"
   },
   "source": [
    "Copyright 2018 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q8ZHSBelKd8V"
   },
   "source": [
    "# Training/Inference on Breast Density Classification Model on Cloud ML Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCClehsPK2Jq"
   },
   "source": [
    "The goal of this tutorial is to train, deploy and run inference on a breast density classification model. Breast density is thought to be a factor for an increase in the risk for breast cancer. This will emphasize using the [Cloud Healthcare API](https://cloud.google.com/healthcare/) in order to store, retreive and transcode medical images (in DICOM format) in a managed and scalable way. This tutorial will focus on using [Cloud Machine Learning Engine](https://cloud.google.com/ml-engine/) to scalably train and serve the model.\n",
    "\n",
    "**Note: This is the Cloud ML Engine version of the AutoML Codelab found [here](./breast_density_auto_ml).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1OUa9pCPtqu"
   },
   "source": [
    "## Requirements\n",
    "- A Google Cloud project.\n",
    "- Project has [Cloud Healthcare API](https://cloud.google.com/healthcare/docs/quickstart) enabled.\n",
    "- Project has [Cloud Machine Learning API ](https://cloud.google.com/ml-engine/docs/tensorflow/getting-started-training-prediction) enabled.\n",
    "- Project has [Cloud Dataflow API ](https://cloud.google.com/dataflow/docs/quickstarts/quickstart-python) enabled.\n",
    "- Project has [Cloud Build API](https://cloud.google.com/cloud-build/docs/quickstart-docker) enabled.\n",
    "- Project has [Kubernetes engine API](https://console.developers.google.com/apis/api/container.googleapis.com/overview?project=) enabled.\n",
    "- Project has [Cloud Resource Manager API](https://console.cloud.google.com/cloud-resource-manager) enabled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qL-vcMmZRITm"
   },
   "source": [
    "## Input Dataset\n",
    "\n",
    "The dataset that will be used for training is the [TCIA CBIS-DDSM](https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM) dataset. This dataset contains ~2500 mammography images in DICOM format. Each image is given a [BI-RADS breast density ](https://breast-cancer.ca/densitbi-rads/) score from 1 to 4. In this tutorial, we will build a binary classifier that distinguishes between breast density \"2\" (*scattered density*) and \"3\" (*heterogeneously dense*). These are the two most common and variably assigned scores. In the literature, this is said to be [particularly difficult for radiologists to consistently distinguish](https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.12683)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vgBA16ptbacM"
   },
   "outputs": [],
   "source": [
    "project_id = \"MY_PROJECT\" # @param\n",
    "location = \"us-central1\"\n",
    "dataset_id = \"MY_DATASET\" # @param\n",
    "dicom_store_id = \"MY_DICOM_STORE\" # @param\n",
    "\n",
    "# Input data used by Cloud ML must be in a bucket with the following format.\n",
    "cloud_bucket_name = \"gs://\" + project_id + \"-vcm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UW3Y6Pd6d1Ey"
   },
   "outputs": [],
   "source": [
    "%%bash -s {project_id} {location} {cloud_bucket_name}\n",
    "# Create bucket.\n",
    "gsutil -q mb -c regional -l $2 $3\n",
    "\n",
    "# Allow Cloud Healthcare API to write to bucket.\n",
    "PROJECT_NUMBER=`gcloud projects describe $1 | grep projectNumber | sed 's/[^0-9]//g'`\n",
    "SERVICE_ACCOUNT=\"service-${PROJECT_NUMBER}@gcp-sa-healthcare.iam.gserviceaccount.com\"\n",
    "COMPUTE_ENGINE_SERVICE_ACCOUNT=\"${PROJECT_NUMBER}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "gsutil -q iam ch serviceAccount:${SERVICE_ACCOUNT}:objectCreator $3\n",
    "gsutil -q iam ch serviceAccount:${COMPUTE_ENGINE_SERVICE_ACCOUNT}:objectCreator $3\n",
    "gcloud projects add-iam-policy-binding $1 --member=serviceAccount:${SERVICE_ACCOUNT} --role=roles/pubsub.publisher\n",
    "gcloud projects add-iam-policy-binding $1 --member=serviceAccount:${COMPUTE_ENGINE_SERVICE_ACCOUNT} --role roles/pubsub.admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vgBA16ptbacM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import google.auth\n",
    "from google.auth.transport.requests import AuthorizedSession\n",
    "\n",
    "credentials, project = google.auth.default()\n",
    "authed_session = AuthorizedSession(credentials)\n",
    "# Path to Cloud Healthcare API.\n",
    "HEALTHCARE_API_URL = 'https://healthcare.googleapis.com/v1beta1'\n",
    "\n",
    "# Create Cloud Healthcare API dataset.\n",
    "path = os.path.join(HEALTHCARE_API_URL, 'projects', project_id, 'locations', location, 'datasets?dataset_id=' + dataset_id)\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "resp = authed_session.post(path, headers=headers)\n",
    "\n",
    "assert resp.status_code == 200, 'error creating Dataset, code: {0}, response: {1}'.format(resp.status_code, resp.text)\n",
    "print('Full response:\\n{0}'.format(resp.text))\n",
    "\n",
    "# Create Cloud Healthcare API DICOM store.\n",
    "path = os.path.join(HEALTHCARE_API_URL, 'projects', project_id, 'locations', location, 'datasets', dataset_id, 'dicomStores?dicom_store_id=' + dicom_store_id)\n",
    "resp = authed_session.post(path, headers=headers)\n",
    "assert resp.status_code == 200, 'error creating DICOM store, code: {0}, response: {1}'.format(resp.status_code, resp.text)\n",
    "print('Full response:\\n{0}'.format(resp.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKsCfbXorosM"
   },
   "source": [
    "Next, we are going to transfer the DICOM instances to the Cloud Healthcare API.\n",
    "\n",
    "Note: We are transfering >100GB of data so this will take some time to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "shhPzNFArpHH"
   },
   "outputs": [],
   "source": [
    "# Store DICOM instances in Cloud Healthcare API.\n",
    "path = \"https://healthcare.googleapis.com/v1beta1/projects/{}/locations/{}/datasets/{}/dicomStores/{}:import\".format(\n",
    "      project_id, location, dataset_id, dicom_store_id)\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "body = { \n",
    "      'gcsSource': {\n",
    "        'uri': 'gs://gcs-public-data--healthcare-tcia-cbis-ddsm/dicom/**'\n",
    "      }\n",
    "}\n",
    "resp = authed_session.post(path, headers=headers, json=body)\n",
    "assert resp.status_code == 200, 'error creating Dataset, code: {0}, response: {1}'.format(resp.status_code, resp.text)\n",
    "print('Full response:\\n{0}'.format(resp.text))\n",
    "response = json.loads(resp.text)\n",
    "operation_name = response['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "biHU0WoLjgYs"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_operation_completion(path, timeout, sleep_time=30): \n",
    "  success = False\n",
    "  while time.time() < timeout:\n",
    "    print('Waiting for operation completion...')\n",
    "    resp = authed_session.get(path)\n",
    "    assert resp.status_code == 200, 'error polling for Operation results, code: {0}, response: {1}'.format(resp.status_code, resp.text)\n",
    "    response = json.loads(resp.text)\n",
    "    if 'done' in response:\n",
    "      if response['done'] == True and 'error' not in response:\n",
    "        success = True;\n",
    "      break\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "  print('Full response:\\n{0}'.format(resp.text))      \n",
    "  assert success, \"operation did not complete successfully in time limit\"\n",
    "  print('Success!')\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(HEALTHCARE_API_URL, operation_name)\n",
    "timeout = time.time() + 40*60 # Wait up to 40 minutes.\n",
    "_ = wait_for_operation_completion(path, timeout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjFm5w356Z6t"
   },
   "source": [
    "### Explore the Cloud Healthcare DICOM dataset (optional)\n",
    "\n",
    "This is an optional section to explore the Cloud Healthcare DICOM dataset. In the following code, we simply just list the studies that we have loaded into the Cloud Healthcare API. You can modify the *num_of_studies_to_print* parameter to print as many studies as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUjYgoym7MZN"
   },
   "outputs": [],
   "source": [
    "num_of_studies_to_print = 2 # @param\n",
    "\n",
    "\n",
    "path = os.path.join(HEALTHCARE_API_URL, 'projects', project_id, 'locations', location, 'datasets', dataset_id, 'dicomStores', dicom_store_id, 'dicomWeb', 'studies')\n",
    "resp = authed_session.get(path)\n",
    "assert resp.status_code == 200, 'error querying Dataset, code: {0}, response: {1}'.format(resp.status_code, resp.text)\n",
    "response = json.loads(resp.text)\n",
    "\n",
    "print(json.dumps(response[:num_of_studies_to_print], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpkeGeAsb-ec"
   },
   "source": [
    "## Convert DICOM to JPEG\n",
    "\n",
    "The ML model that we will build requires that the dataset be in JPEG. We will leverage the Cloud Healthcare API to transcode DICOM to JPEG.\n",
    "\n",
    "First we will create a [Google Cloud Storage](https://cloud.google.com/storage/) bucket to hold the output JPEG files. Next, we will use the ExportDicomData API to transform the DICOMs to JPEGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sXB41cvKeLZj"
   },
   "outputs": [],
   "source": [
    "jpeg_bucket = cloud_bucket_name + \"/images/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIfKnxqHjE9A"
   },
   "source": [
    "Next we will convert the DICOMs to JPEGs using the [ExportDicomData](https://cloud.google.com/sdk/gcloud/reference/beta/healthcare/dicom-stores/export/gcs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Ic_OPeVjM5i"
   },
   "outputs": [],
   "source": [
    "%%bash -s {jpeg_bucket} {project_id} {location} {dataset_id} {dicom_store_id}\n",
    "gcloud beta healthcare --project $2  dicom-stores export gcs $5 --location=$3 --dataset=$4 --mime-type=\"image/jpeg; transfer-syntax=1.2.840.10008.1.2.4.50\" --gcs-uri-prefix=$1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pVhp8IdVkVH3"
   },
   "source": [
    "We will use the Operation name returned from the previous command to poll the status of ExportDicomData. We will poll for operation completeness, which should take a few minutes. When the operation is complete, the operation's *done* field will be set to true.\n",
    "\n",
    "Meanwhile, you should be able to observe the JPEG images being added to your Google Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDxxEp3XnFp1"
   },
   "source": [
    "## Training\n",
    "\n",
    "We will use [Transfer Learning](https://en.wikipedia.org/wiki/Transfer_learning) to retrain a generically trained trained model to perform breast density classification. Specifically, we will use an [Inception V3](https://github.com/tensorflow/models/tree/master/research/inception) checkpoint as the starting point.\n",
    "\n",
    "The neural network we will use can roughly be split into two parts: \"feature extraction\" and \"classification\". In transfer learning, we take advantage of a pre-trained (checkpoint) model to do the \"feature extraction\", and add a few layers to perform the \"classification\" relevant to the specific problem. In this case, we are adding aa [dense](https://www.tensorflow.org/api_docs/python/tf/layers/dense) layer with two neurons to do the classification and a [softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) layer to normalize the classification score.  The mammography images will be classified as either \"2\" (scattered density) or \"3\" (heterogeneously dense). See below for diagram of the training process:\n",
    "\n",
    "![Inception V3](images/cloud_ml_training_pipeline.png)\n",
    "\n",
    "\n",
    "The \"feature extraction\" and the \"classification\" part will be done in the following steps, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NsamYTK9q07p"
   },
   "source": [
    "### Preprocess Raw Images using Cloud Dataflow\n",
    "\n",
    "In this step, we will resize images to 300x300 (required for Inception V3) and will run each image through the checkpoint Inception V3 model to calculate the *bottleneck values*. This is the feature vector for the output of the feature extraction part of the model (the part that is already pre-trained). Since this process is resource intensive, we will utilize [Cloud Dataflow](https://cloud.google.com/dataflow/) in order to do this scalably. We extract the features and calculate the bottleneck values here for performance reasons - so that we don't have to recalculate them during training.\n",
    "\n",
    "The output of this process will be a collection of [TFRecords](https://www.tensorflow.org/guide/datasets) storing the bottleneck value for each image in the input dataset. This TFRecord format is commonly used to store Tensors in binary format for storage.\n",
    "\n",
    "Finally, in this step, we will also split the input dataset into *training*, *validation* or *testing*. The percentage of each can be modified using the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wypDk298yO3l"
   },
   "outputs": [],
   "source": [
    "# GCS Bucket to store output TFRecords.\n",
    "bottleneck_bucket = cloud_bucket_name + \"/bottleneck/\" # @param\n",
    "\n",
    "# Percentage of dataset to allocate for validation and testing.\n",
    "validation_percentage = 10 # @param\n",
    "testing_percentage = 10 # @param\n",
    "\n",
    "# Number of Dataflow workers. This can be increased to improve throughput.\n",
    "dataflow_num_workers = 5 # @param\n",
    "\n",
    "# Staging bucket for training.\n",
    "staging_bucket = cloud_bucket_name # @param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJvhrerQmgE4"
   },
   "source": [
    "The following command will kick off a Cloud Dataflow pipeline that runs preprocessing. The script that has the relevant code is [preprocess.py](./scripts/trainer/preprocess.py). ***You can check out how the pipeline is progressing [here](https://console.cloud.google.com/dataflow)***.\n",
    "\n",
    "When the operation is done, we will begin training the classification layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U3Rz2Cz9z2P7"
   },
   "outputs": [],
   "source": [
    "%%bash -s {project_id} {jpeg_bucket} {bottleneck_bucket} {validation_percentage} {testing_percentage} {dataflow_num_workers} {staging_bucket}\n",
    "\n",
    "# Install Python library dependencies.\n",
    "sudo pip3 install  tensorflow==1.15.0 google-apitools apache_beam[gcp]==2.18.0 --ignore-installed\n",
    "# Start job in Cloud Dataflow and wait for completion.\n",
    "python3 -m scripts.preprocess.preprocess \\\n",
    "    --project $1 \\\n",
    "    --input_path $2 \\\n",
    "    --output_path \"$3/record\" \\\n",
    "    --num_workers $6 \\\n",
    "    --temp_location \"$7/temp\" \\\n",
    "    --staging_location \"$7/staging\" \\\n",
    "    --validation_percentage $4 \\\n",
    "    --testing_percentage $5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s08RCopGrEqx"
   },
   "source": [
    "### Train the Classification Layers of Model using Cloud ML Engine\n",
    "\n",
    "In this step, we will train the classification layers of the model. This consists of just a [dense](https://www.tensorflow.org/api_docs/python/tf/layers/dense) and [softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) layer. We will use the bottleneck values calculated at the previous step as the input to these layers. We will use Cloud ML Engine to train the model. The output of stage will be a trained model exported to GCS, which can be used for inference.\n",
    "\n",
    "\n",
    "There are various training parameters below that can be tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVml9yNuHnzf"
   },
   "outputs": [],
   "source": [
    "training_steps = 1000 # @param\n",
    "learning_rate = 0.01 # @param\n",
    "\n",
    "# Location of exported model.\n",
    "exported_model_bucket = cloud_bucket_name + \"/models\" # @param\n",
    "\n",
    "\n",
    "# Inference requires the exported model to be versioned (by default we choose version 1).\n",
    "exported_model_versioned_uri = exported_model_bucket + \"/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ub-wgmOwKvm0"
   },
   "source": [
    "We'll invoke Cloud ML Engine with the above parameters. We use a GPU for training to speed up operations. The script that does the training is [model.py](./scripts/trainer/model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4LzKBDbK953"
   },
   "outputs": [],
   "source": [
    "%%bash -s {location} {bottleneck_bucket} {staging_bucket} {training_steps} {learning_rate} {exported_model_versioned_uri}\n",
    "\n",
    "# Start training on CMLE.\n",
    "gcloud ai-platform jobs submit training breast_density \\\n",
    "    --python-version 3.5 \\\n",
    "    --runtime-version 1.14 \\\n",
    "    --scale-tier BASIC_GPU \\\n",
    "    --module-name \"scripts.trainer.model\" \\\n",
    "    --package-path scripts \\\n",
    "    --staging-bucket $3 \\\n",
    "    --region $1 \\\n",
    "    -- \\\n",
    "    --bottleneck_dir \"$2/record\" \\\n",
    "    --training_steps $4 \\\n",
    "    --learning_rate $5 \\\n",
    "    --export_model_path $6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mVUV9GlNI3e"
   },
   "source": [
    "You can monitor the status of the training job by running the following command. The job can take a few minutes to start-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgSKpZYCNPVD"
   },
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs describe breast_density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "os3RAC17Nsjo"
   },
   "source": [
    "When the job has started, you can observe the logs for the training job by executing the below command (it will poll for new logs every 30 seconds).\n",
    "\n",
    "As training progresses, the logs will output the accuracy on the training set, validation set, as well as the [cross entropy](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html). You'll generally see that the accuracy goes up, while the cross entropy goes down as the number of training iterations increases.\n",
    "\n",
    "\n",
    "Finally, when the training is complete, the accuracy of the model on the held-out test set will be output to console. The job can take a few minutes to shut-down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbqGVOvoOPkc"
   },
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs stream-logs breast_density --polling-interval=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j2NI-ILU9HoN"
   },
   "source": [
    "### Export Trained Model for Inference in Cloud ML Engine\n",
    "\n",
    "Cloud ML Engine can also be used to serve the model for inference.  The inference model is composed of the pre-trained Inception V3 checkpoint, along with the classification layers we trained above for breast density. First we set the inference model name and version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HvniHAbZGeVp"
   },
   "outputs": [],
   "source": [
    "model_name = \"breast_density\" # @param\n",
    "version = \"v1\" # @param\n",
    "\n",
    "# The full name of the model.\n",
    "full_model_name = \"projects/\" + project_id + \"/models/\" + model_name + \"/versions/\" + version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wY-h7vnmHLZ7"
   },
   "outputs": [],
   "source": [
    "!gcloud ai-platform models create $model_name --regions $location\n",
    "!gcloud ai-platform versions create $version --model $model_name --origin $exported_model_versioned_uri --runtime-version 1.14 --python-version 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYbmGGoooIws"
   },
   "source": [
    "## Inference\n",
    "\n",
    "To allow medical imaging ML models to be easily integrated into clinical workflows, an *inference module* can be used. A standalone modality, a PACS system or a DICOM router can push DICOM instances into Cloud Healthcare [DICOM stores](https://cloud.google.com/healthcare/docs/introduction), allowing ML models to be triggered for inference. This inference results can then be structured into various DICOM formats (e.g. DICOM [structured reports](http://dicom.nema.org/MEDICAL/Dicom/2014b/output/chtml/part20/sect_A.3.html)) and stored in the Cloud Healthcare API, which can then be retrieved by the customer.\n",
    "\n",
    "The inference module is built as a [Docker](https://www.docker.com/) container and deployed using [Kubernetes](https://kubernetes.io/), allowing you to easily scale your deployment. The dataflow for inference can look as follows (see corresponding diagram below):\n",
    "\n",
    "1. Client application uses [STOW-RS](ftp://dicom.nema.org/medical/Dicom/2013/output/chtml/part18/sect_6.6.html) to push a new DICOM instance to the Cloud Healthcare DICOMWeb API.\n",
    "\n",
    "2. The insertion of the DICOM instance triggers a [Cloud Pubsub](https://cloud.google.com/pubsub/) message to be published. The *inference module* will pull incoming Pubsub messages and will recieve a message for the previously inserted DICOM instance. \n",
    "\n",
    "3. The *inference module* will retrieve the instance in JPEG format from the Cloud Healthcare API using [WADO-RS](ftp://dicom.nema.org/medical/Dicom/2013/output/chtml/part18/sect_6.5.html).\n",
    "\n",
    "4. The *inference module* will send the JPEG bytes to the model hosted on Cloud ML Engine.\n",
    "\n",
    "5. Cloud ML Engine will return the prediction back to the  *inference module*.\n",
    "\n",
    "6. The *inference module* will package the prediction into a DICOM instance. This can potentially be a DICOM structured report, [presentation state](ftp://dicom.nema.org/MEDICAL/dicom/2014b/output/chtml/part03/sect_A.33.html), or even burnt text on the image. In this codelab, we will focus on just DICOM structured reports. The structured report is then stored back in the Cloud Healthcare API using STOW-RS.\n",
    "\n",
    "7. The client application can query for (or retrieve) the structured report by using [QIDO-RS](http://dicom.nema.org/dicom/2013/output/chtml/part18/sect_6.7.html) or WADO-RS. Pubsub can also be used by the client application to poll for the newly created DICOM structured report instance.\n",
    "\n",
    "![Inference data flow](images/cloud_ml_inference_pipeline.png)\n",
    "\n",
    "\n",
    "To begin, we will create a new DICOM store that will store our inference source (DICOM mammography instance) and results (DICOM structured report). In order to enable Pubsub notifications to be triggered on inserted instances, we will give the DICOM store a Pubsub channel to publish on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7VrAx6B0TQK"
   },
   "outputs": [],
   "source": [
    "# Pubsub config.\n",
    "pubsub_topic_id = \"MY_PUBSUB_TOPIC_ID\" # @param\n",
    "pubsub_subscription_id = \"MY_PUBSUB_SUBSRIPTION_ID\" # @param\n",
    "\n",
    "# DICOM Store for store DICOM used for inference.\n",
    "inference_dicom_store_id = \"MY_INFERENCE_DICOM_STORE\" # @param\n",
    "\n",
    "pubsub_subscription_name = \"projects/\" + project_id + \"/subscriptions/\" + pubsub_subscription_id\n",
    "inference_dicom_store_name = \"projects/\" + project_id + \"/locations/\" + location + \"/datasets/\" + dataset_id + \"/dicomStores/\" + inference_dicom_store_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v10GgraT7XbO"
   },
   "outputs": [],
   "source": [
    "%%bash -s {pubsub_topic_id} {pubsub_subscription_id} {project_id} {location} {dataset_id} {inference_dicom_store_id}\n",
    "\n",
    "# Create Pubsub channel.\n",
    "gcloud beta pubsub topics create $1\n",
    "gcloud beta pubsub subscriptions create $2 --topic $1\n",
    "\n",
    "# Create a Cloud Healthcare DICOM store that published on given Pubsub topic.\n",
    "TOKEN=`gcloud beta auth application-default print-access-token`\n",
    "NOTIFICATION_CONFIG=\"{notification_config: {pubsub_topic: \\\"projects/$3/topics/$1\\\"}}\"\n",
    "curl -s -X POST -H \"Content-Type: application/json\" -d \"${NOTIFICATION_CONFIG}\" https://healthcare.googleapis.com/v1beta1/projects/$3/locations/$4/datasets/$5/dicomStores?access_token=${TOKEN}\\&dicom_store_id=$6\n",
    "\n",
    "# Enable Cloud Healthcare API to publish on given Pubsub topic.\n",
    "PROJECT_NUMBER=`gcloud projects describe $3 | grep projectNumber | sed 's/[^0-9]//g'`\n",
    "SERVICE_ACCOUNT=\"service-${PROJECT_NUMBER}@gcp-sa-healthcare.iam.gserviceaccount.com\"\n",
    "gcloud beta pubsub topics add-iam-policy-binding $1 --member=\"serviceAccount:${SERVICE_ACCOUNT}\" --role=\"roles/pubsub.publisher\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VRs-EWOf_VIU"
   },
   "source": [
    "Next, we will building the *inference module* using [Cloud Build API](https://cloud.google.com/cloud-build/docs/api/reference/rest/). This will create a Docker container that will be stored in [Google Container Registry](https://cloud.google.com/container-registry/). The inference module code is found in *[inference.py](./scripts/inference/inference.py)*. The build script used to build the Docker container for this module is *[cloudbuild.yaml](./scripts/inference/cloudbuild.yaml)*. Progress of build may be found on [cloud build dashboard](https://console.cloud.google.com/cloud-build/builds?project=)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nolumVGiL47X"
   },
   "outputs": [],
   "source": [
    "%%bash -s {project_id}\n",
    "PROJECT_ID=$1\n",
    "\n",
    "gcloud builds submit --config scripts/inference/cloudbuild.yaml --timeout 1h scripts/inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YtufvIOrdnP7"
   },
   "source": [
    "Next, we will deploy the *inference module* to Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIWLkRKleFJS"
   },
   "source": [
    "Then we create a Kubernetes Cluster and a Deployment for the *inference module*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJHRFPvjeDnZ"
   },
   "outputs": [],
   "source": [
    "%%bash -s {project_id} {location} {pubsub_subscription_name} {full_model_name} {inference_dicom_store_name}\n",
    "gcloud container clusters create inference-module --region=$2 --scopes https://www.googleapis.com/auth/cloud-platform --num-nodes=1\n",
    "\n",
    "PROJECT_ID=$1\n",
    "SUBSCRIPTION_PATH=$3\n",
    "MODEL_PATH=$4\n",
    "INFERENCE_DICOM_STORE_NAME=$5\n",
    "\n",
    "cat <<EOF | kubectl create -f -\n",
    "apiVersion: extensions/v1beta1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: inference-module\n",
    "  namespace: default\n",
    "spec:\n",
    "  replicas: 1\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: inference-module\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: inference-module\n",
    "          image: gcr.io/${PROJECT_ID}/inference-module:latest\n",
    "          command:\n",
    "            - \"/opt/inference_module/bin/inference_module\"\n",
    "            - \"--subscription_path=${SUBSCRIPTION_PATH}\"\n",
    "            - \"--model_path=${MODEL_PATH}\"\n",
    "            - \"--dicom_store_path=${INFERENCE_DICOM_STORE_NAME}\"\n",
    "            - \"--prediction_service=CMLE\"\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dgUu3dJN8spl"
   },
   "source": [
    "Next, we will store a mammography DICOM instance from the TCIA dataset to the DICOM store. This is the image that we will request inference for. Pushing this instance to the DICOM store will result in a Pubsub message, which will trigger the *inference module*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9CG0225T8rw2"
   },
   "outputs": [],
   "source": [
    "# DICOM Study/Series UID of input mammography image that we'll push for inference.\n",
    "input_mammo_study_uid = \"1.3.6.1.4.1.9590.100.1.2.85935434310203356712688695661986996009\"\n",
    "input_mammo_series_uid = \"1.3.6.1.4.1.9590.100.1.2.374115997511889073021386151921807063992\"\n",
    "input_mammo_instance_uid = \"1.3.6.1.4.1.9590.100.1.2.289923739312470966435676008311959891294\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SRtBZf5N-ou8"
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket('gcs-public-data--healthcare-tcia-cbis-ddsm', user_project=project_id)\n",
    "blob = bucket.blob(\"dicom/{}/{}/{}.dcm\".format(input_mammo_study_uid,input_mammo_series_uid,input_mammo_instance_uid))\n",
    "blob.download_to_filename(\"example.dcm\")\n",
    "with open(\"example.dcm\", 'rb') as dcm:\n",
    "  dcm_content = dcm.read()\n",
    "path = os.path.join(HEALTHCARE_API_URL, inference_dicom_store_name, 'dicomWeb', 'studies')\n",
    "headers = {'Content-Type': 'application/dicom'}\n",
    "authed_session.post(path, headers=headers, data=dcm_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SrPN8rW1wxcg"
   },
   "source": [
    "You should be able to observe the *inference module*'s logs by running the following command. In the logs, you should observe that the inference module successfully recieved the the Pubsub message and ran inference on the DICOM instance. The logs should also include the inference results. It can take a few minutes to start-up the Kubernetes deployment, so you many have to run this a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "muiDpFTuxMOk"
   },
   "outputs": [],
   "source": [
    "!kubectl logs -l app=inference-module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9ibc2yayM_j"
   },
   "source": [
    "You can also query the Cloud Healthcare DICOMWeb API (using QIDO-RS) to see that the DICOM structured report has been inserted for the study. The structured report contents can be found under tag **\"0040A730\"**. \n",
    "\n",
    "You can optionally also use WADO-RS to recieve the instance (e.g. for viewing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grWtbIQCyL0r"
   },
   "outputs": [],
   "source": [
    "%%bash -s {project_id} {location} {dataset_id} {inference_dicom_store_id} {input_mammo_study_uid}\n",
    "\n",
    "TOKEN=`gcloud beta auth application-default print-access-token`\n",
    "\n",
    "# QIDO-RS should return two results in JSON response. One for the original DICOM\n",
    "# instance, and one for the Strucured Report containing the inference results.\n",
    "curl -s https://healthcare.googleapis.com/v1beta1/projects/$1/locations/$2/datasets/$3/dicomStores/$4/dicomWeb/studies/$5/instances?includefield=all\\&access_token=${TOKEN} | python -m json.tool"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "breast_density_cloud_ml.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
